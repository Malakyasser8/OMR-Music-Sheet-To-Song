{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OMR Music Sheet to A MP3 file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.io as io\n",
    "from skimage import img_as_ubyte\n",
    "import skimage as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage.exposure import histogram\n",
    "from skimage.filters import threshold_otsu,threshold_local,median, unsharp_mask\n",
    "from matplotlib.pyplot import bar\n",
    "from skimage.color import rgb2gray,rgb2hsv\n",
    "from commonfunctions import *\n",
    "from scipy.ndimage import rotate\n",
    "from skimage.morphology import binary_erosion, binary_dilation, binary_closing, skeletonize, thin, binary_opening, remove_small_objects\n",
    "from skimage.measure import find_contours\n",
    "from skimage.draw import rectangle\n",
    "from skimage.transform import resize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images,titles=None):\n",
    "    #This function is used to show image(s) with titles by sending an array of images and an array of associated titles.\n",
    "    # images[0] will be drawn with the title titles[0] if exists\n",
    "    # You aren't required to understand this function, use it as-is.\n",
    "    n_ims = len(images)\n",
    "    if titles is None: titles = ['(%d)' % i for i in range(1,n_ims + 1)]\n",
    "    fig = plt.figure()\n",
    "    n = 1\n",
    "    for image,title in zip(images,titles):\n",
    "        a = fig.add_subplot(1,n_ims,n)\n",
    "        if image.ndim == 2: \n",
    "            plt.gray()\n",
    "        plt.imshow(image)\n",
    "        a.set_title(title)\n",
    "        n += 1\n",
    "    fig.set_size_inches(np.array(fig.get_size_inches()) * n_ims)\n",
    "    plt.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram_equation(image):\n",
    "    shape=image.shape\n",
    "    H=np.zeros(256)\n",
    "    for x in range(0,shape[0]):\n",
    "        for y in range(0,shape[1]):\n",
    "            H[image[x][y]]+=1\n",
    "\n",
    "    return H\n",
    "\n",
    "\n",
    "def getThresholdRawan(img):\n",
    "    counts, _ = np.histogram(img, bins=256, range=(0, 256))\n",
    "    \n",
    "    total_pixels = np.sum(counts)\n",
    "    print(total_pixels)\n",
    "    Tinit = int(round(np.sum([i * counts[i] for i in range(256)]) / total_pixels))\n",
    "\n",
    "    \n",
    "    while True:\n",
    "        if np.sum(counts[:Tinit]) == 0 or np.sum(counts[Tinit:]) == 0:\n",
    "            return Tinit\n",
    "        lower = np.sum([i * counts[i] for i in range(Tinit)]) / np.sum(counts[:Tinit])\n",
    "        higher = np.sum([i * counts[i] for i in range(Tinit, 256)]) / np.sum(counts[Tinit:])\n",
    "        new_T = int((lower + higher) / 2)\n",
    "        if new_T == Tinit:\n",
    "            return new_T\n",
    "        Tinit = new_T\n",
    "\n",
    "def partition_imageRawan(img, n_partitions_horiz, n_partitions_vert):\n",
    "    print(img.shape)\n",
    "    height, width = img.shape\n",
    "    row_step = height // n_partitions_horiz\n",
    "    col_step = width // n_partitions_vert\n",
    "    \n",
    "    binary_img = np.zeros_like(img)\n",
    "    \n",
    "    for i in range(n_partitions_horiz):\n",
    "        for j in range(n_partitions_vert):\n",
    "            row_start = i * row_step\n",
    "            row_end = (i + 1) * row_step if i < n_partitions_horiz - 1 else height\n",
    "            col_start = j * col_step\n",
    "            col_end = (j + 1) * col_step if j < n_partitions_vert - 1 else width\n",
    "            \n",
    "            img_section = img[row_start:row_end, col_start:col_end]\n",
    "            threshold = getThresholdRawan(img_section)\n",
    "            binary_section = img_section > threshold\n",
    "            binary_img[row_start:row_end, col_start:col_end] = binary_section\n",
    "            \n",
    "    return binary_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getThreshold(image):\n",
    "    # image = (image * 255).astype('uint8')\n",
    "\n",
    "    counts, _ = np.histogram(image, bins=256, range=(0, 256))\n",
    "    \n",
    "    total_pixels = counts.sum()\n",
    "    gray_levels = np.arange(256)\n",
    "    Tinit = round((gray_levels * counts).sum() / total_pixels) \n",
    "    \n",
    "    while True:\n",
    "        lower_pixels = counts[:Tinit]\n",
    "        upper_pixels = counts[Tinit:]\n",
    "        \n",
    "        if lower_pixels.sum() == 0 or upper_pixels.sum() == 0:\n",
    "            break\n",
    "\n",
    "        lower_mean = (gray_levels[:Tinit] * lower_pixels).sum() / lower_pixels.sum()\n",
    "        upper_mean = (gray_levels[Tinit:] * upper_pixels).sum() / upper_pixels.sum()\n",
    "\n",
    "        new_threshold = round((lower_mean + upper_mean) / 2)\n",
    "        \n",
    "        if new_threshold == Tinit:\n",
    "            break\n",
    "        \n",
    "        Tinit = new_threshold\n",
    "    \n",
    "    binary_image = np.where(image < Tinit, 0, 255) # 0 is black, 255 is white\n",
    "    return binary_image\n",
    "\n",
    "\n",
    "def adaptive (image, NumberOfDivisions):\n",
    "    new_image = np.array_split(image, NumberOfDivisions, axis=1)\n",
    "    thresholded_images = []\n",
    "    for imagee in new_image:\n",
    "        threshold = getThreshold(imagee)\n",
    "        thresholded_images.append(threshold)    \n",
    "    \n",
    "    combined_image = np.block(thresholded_images)\n",
    "\n",
    "    return combined_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing\n",
    "def deskew(original_img):\n",
    "    img = np.copy((original_img))\n",
    "    # Canny\n",
    "    imgCanny = sk.feature.canny(img, sigma=1.5)\n",
    "    thresh = sk.filters.threshold_otsu(imgCanny)\n",
    "    imgCanny = (imgCanny >= thresh)\n",
    "\n",
    "    # Apply Hough Transform\n",
    "    # Generates a list of 360 Radian degrees (-pi/2, pi/2)\n",
    "    angleSet = np.linspace(-np.pi, np.pi, 1440)\n",
    "    houghArr, theta, dis = sk.transform.hough_line(imgCanny, angleSet)\n",
    "\n",
    "    flatIdx = np.argmax(houghArr)\n",
    "    bestTheta = (flatIdx % theta.shape[0])\n",
    "    bestTheta = angleSet[bestTheta]\n",
    "    bestDis = np.int32(np.floor(flatIdx / theta.shape[0]))\n",
    "    bestDis = dis[bestDis]\n",
    "\n",
    "    # Rotate\n",
    "    thetaRotateDeg = (bestTheta*180)/np.pi\n",
    "    if thetaRotateDeg > 0:\n",
    "        thetaRotateDeg = thetaRotateDeg - 90\n",
    "    else:\n",
    "        thetaRotateDeg = thetaRotateDeg + 90\n",
    "\n",
    "    imgRotated = (sk.transform.rotate(\n",
    "        img, thetaRotateDeg, resize=True, mode='constant', cval=1))\n",
    "    return imgRotated\n",
    "\n",
    "def binarize(img, block_size=101):\n",
    "    t = sk.filters.threshold_local(img, block_size, offset=10)\n",
    "    img_b = img < t\n",
    "    return img_b\n",
    "\n",
    "\n",
    "\n",
    "def convertImgToUINT8(img_o):\n",
    "    img = np.copy(img_o)\n",
    "    img = img.astype(np.float64) / np.max(img)\n",
    "    img = 255 * img\n",
    "    img = img.astype(np.uint8)\n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "img = io.imread(\"./images/image2.png\",as_gray=True)\n",
    "if img.shape[0] > 1000 or img.shape[1] > 2000:\n",
    "        img = sk.transform.resize(img, (img.shape[0]//4, img.shape[1]//4))\n",
    "img1=deskew(img)\n",
    "img2 = convertImgToUINT8(img1)\n",
    "img3=binarize(img2)\n",
    "show_images([img3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = io.imread(\"./images/image.png\",as_gray=True)\n",
    "print(img.shape)\n",
    "# 1. Binarization\n",
    "\n",
    "# # Compute Otsu's threshold\n",
    "# otsu_threshold = threshold_otsu(img)\n",
    "# binary_image_ski = img > otsu_threshold\n",
    "\n",
    "#Adaptive ski\n",
    "# # Compute a local threshold\n",
    "# block_size = 201  # Size of the local region to consider\n",
    "# local_thresh = threshold_local(img, block_size, offset=0)\n",
    "\n",
    "#our threshold\n",
    "# print(img)\n",
    "# threshold=getThreshold(img)\n",
    "# img[img>threshold]=1\n",
    "# img[img<=threshold]=0\n",
    "\n",
    "show_images([img])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_binarization(input_image):\n",
    "    # Calculate the histogram of the input image\n",
    "    hist = cv2.calcHist([input_image], [0], None, [256], [0, 256])\n",
    "    # Normalize the histogram for easier analysis\n",
    "    normalized_hist = hist / hist.sum()\n",
    "    \n",
    "    # Check if the histogram has significant peaks near 0 and 255\n",
    "    near_zero = normalized_hist[:20].sum()  # Sum of bins near 0\n",
    "    near_255 = normalized_hist[-20:].sum()  # Sum of bins near 255\n",
    "\n",
    "    # Thresholds for determining scanned images\n",
    "    if near_zero+near_255 > 0.75:\n",
    "        # Use Otsu's binarization for scanned images\n",
    "        _, binarized_image = cv2.threshold(input_image, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    else:\n",
    "        # Apply both Otsu and Adaptive Thresholding\n",
    "        _, otsu_binary = cv2.threshold(input_image, 0, 255,cv2.THRESH_BINARY+ cv2.THRESH_OTSU)\n",
    "        adaptive_binary = cv2.adaptiveThreshold(input_image, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "                                                cv2.THRESH_BINARY, blockSize=9, C=5)\n",
    "        \n",
    "        # Calculate the number of black pixels for Otsu's output\n",
    "        otsu_black_pixels = np.sum(otsu_binary == 0)\n",
    "\n",
    "        # Threshold for deciding noise\n",
    "        noise_threshold = np.sum(hist[:120])\n",
    "\n",
    "        if otsu_black_pixels > noise_threshold:\n",
    "            # Otsu produces noise, use Adaptive Thresholding\n",
    "            binarized_image = adaptive_binary\n",
    "        else:\n",
    "            # Otsu works fine\n",
    "            binarized_image = otsu_binary\n",
    "\n",
    "    return binarized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def resize_with_aspect_ratio(image, target_width, target_height, color=0):\n",
    "    # Get original dimensions\n",
    "    h, w = image.shape[:2]\n",
    "    \n",
    "    # Calculate the scaling factor to maintain aspect ratio\n",
    "    scale = min(target_width / w, target_height / h)\n",
    "    \n",
    "    # Compute the new dimensions\n",
    "    new_width = int(w * scale)\n",
    "    new_height = int(h * scale)\n",
    "    \n",
    "    # Resize the image\n",
    "    resized_image = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # Create a new blank image with the target dimensions\n",
    "    canvas =np.full((target_height, target_width), color, dtype=np.uint8)\n",
    "    \n",
    "    # Calculate padding to center the resized image\n",
    "    x_offset = (target_width - new_width) // 2\n",
    "    y_offset = (target_height - new_height) // 2\n",
    "    \n",
    "    # Place the resized image onto the canvas\n",
    "    canvas[y_offset:y_offset + new_height, x_offset:x_offset + new_width] = resized_image\n",
    "    \n",
    "    return canvas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the image\n",
    "img = cv2.imread(\"./images/image15.png\")\n",
    "\n",
    "#remove noise using ski and opencv filters\n",
    "# remove_noise = median(img)\n",
    "remove_noise = cv2.bilateralFilter(img, 3, 75, 75)\n",
    "\n",
    "# show_images([remove_noise])\n",
    "\n",
    "# Convert to grayscale\n",
    "gray = cv2.cvtColor(remove_noise, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#dynamic_binarization to apply any of otsu or adaptive\n",
    "binarized_image=dynamic_binarization(gray)\n",
    "\n",
    "# show_images([binarized_image],[\"binarized_image\"])\n",
    "\n",
    "edges = cv2.Canny(gray, 50, 150)\n",
    "\n",
    "#detect lines using Hough Transform\n",
    "lines = cv2.HoughLines(edges, 1, np.pi / 180.0, 250, np.array([]))\n",
    "\n",
    "rotated_image=binarized_image\n",
    "\n",
    "if lines is not None:  # Found lines\n",
    "    angles = []  # Store the angles of the detected lines\n",
    "    zero_angle_count = 0  # Count lines with angles close to 0°\n",
    "    tolerance_zero = 2.0  # Tolerance to consider an angle as 0° (degrees)\n",
    "\n",
    "    for line in lines:\n",
    "        rho, theta = line[0]\n",
    "        # Convert polar coordinates to Cartesian line endpoints\n",
    "        a = np.cos(theta)\n",
    "        b = np.sin(theta)\n",
    "        x0 = a * rho\n",
    "        y0 = b * rho\n",
    "        x1 = int(x0 + 1000 * (-b))  # Endpoint 1\n",
    "        y1 = int(y0 + 1000 * (a))\n",
    "        x2 = int(x0 - 1000 * (-b))  # Endpoint 2\n",
    "        y2 = int(y0 - 1000 * (a))\n",
    "\n",
    "        # # Draw the line on the output image\n",
    "        # cv2.line(output_image, (x1, y1), (x2, y2), (0, 0, 255), 2)  # Red line with thickness 2\n",
    "\n",
    "        # Convert angle to degrees\n",
    "        angle_degrees = 180 * theta / np.pi\n",
    "        angles.append(angle_degrees)\n",
    "\n",
    "        # Check if angle is close to 0° (horizontal line)\n",
    "        if abs(angle_degrees) < tolerance_zero or abs(angle_degrees - 180) < tolerance_zero:\n",
    "            zero_angle_count += 1\n",
    "\n",
    "    # Majority Check: If most lines are at zero angles, skip rotation\n",
    "    if zero_angle_count > len(angles) / 2:  # Majority condition\n",
    "        print(\"Majority of lines are horizontal. Skipping rotation.\")\n",
    "    else:\n",
    "        # Calculate the average angle excluding near-zero angles\n",
    "        average_angle = np.mean([angle for angle in angles if abs(angle) > tolerance_zero])\n",
    "        if(average_angle - 90)>2:\n",
    "            print(f\"Rotating by {average_angle - 90:.2f} degrees\")\n",
    "            rotated_image = rotate(rotated_image, average_angle - 90, cval=255)\n",
    "\n",
    "else:\n",
    "    print(\"No lines found\")\n",
    "\n",
    "\n",
    "rotated_image=255-rotated_image\n",
    "rotated_image=rotated_image>0\n",
    "final_image=remove_small_objects(rotated_image, 700, connectivity=8)\n",
    "\n",
    "show_images([final_image],[\"final_image\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STAVE EXTRACTION\n",
    "1. Apply run length encoding to get staff height and staff space\n",
    "2. vote for each row if it contains a staff to get staff rows\n",
    "3. segment the image according to this rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runs_of_ones_array(bits):\n",
    "    bounded = np.hstack(([0], bits, [0]))\n",
    "    difs = np.diff(bounded)\n",
    "    run_starts, = np.where(difs > 0)\n",
    "    run_ends, = np.where(difs < 0)\n",
    "    return run_ends - run_starts\n",
    "\n",
    "\n",
    "\n",
    "def verticalRunLength(img):\n",
    "    # white runs\n",
    "    arr = []\n",
    "    for i in range(0, img.shape[1]):\n",
    "        a = runs_of_ones_array(img[:, i])\n",
    "        for x in a:\n",
    "            arr.append(x)\n",
    "    counts = np.bincount(arr)\n",
    "    staff_height = np.argmax(counts)\n",
    "    # black runs\n",
    "    arr = []\n",
    "    for i in range(0, img.shape[1]):\n",
    "        a = runs_of_ones_array(np.invert(img[:, i]))\n",
    "        for x in a:\n",
    "            arr.append(x)\n",
    "    # print(arr)\n",
    "    counts = np.bincount(arr)\n",
    "    staff_space = np.argmax(counts)\n",
    "    return staff_height, staff_space\n",
    "\n",
    "def get_lines_rows(img, T_LEN):\n",
    "    row_start_freq = np.zeros((1, img.shape[0]+5))[0]\n",
    "    row_starts = []\n",
    "\n",
    "    for i in range(0, img.shape[1]):\n",
    "        arr = runs_of_ones_array(img[:, i])\n",
    "        k = 0\n",
    "        j = 0\n",
    "        while j < img.shape[0]:\n",
    "            if img[j][i] == True:\n",
    "                if arr[k] <= T_LEN + 2 and arr[k] >= T_LEN - 2:\n",
    "                    row_start_freq[j] += 1\n",
    "                    j += arr[k]-1\n",
    "                else:\n",
    "                    j += arr[k]\n",
    "\n",
    "                k += 1\n",
    "            j += 1\n",
    "\n",
    "    max_freq_row_start = 0\n",
    "    for r in row_start_freq:\n",
    "        max_freq_row_start = max(max_freq_row_start, r)\n",
    "\n",
    "    for i in range(len(row_start_freq)):\n",
    "        # Approximately, if the row \"i\" is frequently treated as a starting of staffs with this ratio\n",
    "        # by the most frequnt starting row, then consider it as a starting row of staffs.\n",
    "        if row_start_freq[i]/max_freq_row_start >= 0.12:\n",
    "            row_starts.append(i)\n",
    "    return [row_starts, row_start_freq, max_freq_row_start]\n",
    "\n",
    "\n",
    "print(\"final_image\",final_image)\n",
    "show_images([final_image],[\"final_image\"])\n",
    "\n",
    "\n",
    "staff_height,staff_space = verticalRunLength(final_image)\n",
    "print(\"staff_height\",staff_height)\n",
    "print(\"staff_space\",staff_space)\n",
    "\n",
    "staff_rows,_,__=get_lines_rows(final_image,staff_height)\n",
    "print(\"staff_rows\",staff_rows)\n",
    "\n",
    "staves=[]\n",
    "for i in range(0,len(staff_rows),5):\n",
    "    staves.append(final_image[staff_rows[i]-staff_space*2:staff_rows[i+4]+staff_space*2,:])\n",
    "\n",
    "for i in range(len(staves)):\n",
    "    show_images([staves[i]],[\"stave\"+str(i)])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_staff_lines_columnwise(staves, staff_height):\n",
    "    processed_staves = []\n",
    "\n",
    "    for stave in staves:\n",
    "        # Copy the stave to modify\n",
    "        processed_stave = stave.copy()\n",
    "\n",
    "        # Loop through each column in the stave\n",
    "        for col in range(stave.shape[1]):\n",
    "            # Get the binary column data\n",
    "            column_data = stave[:, col]\n",
    "\n",
    "            # Run-length encode the column\n",
    "            run_lengths = runs_of_ones_array(column_data)\n",
    "\n",
    "            # Check if any gap (run of black pixels) is larger than the staff height\n",
    "            if any(run > staff_height*2 for run in run_lengths):\n",
    "                # Leave the column as is (it contains symbols or non-staff lines)\n",
    "                continue\n",
    "            else:\n",
    "                # Set the entire column to black (remove staff lines)\n",
    "                processed_stave[:, col] = 0\n",
    "\n",
    "        # Append the processed stave\n",
    "        processed_staves.append(processed_stave)\n",
    "\n",
    "    return processed_staves\n",
    "\n",
    "# Process each stave using the refined column-wise approach\n",
    "processed_staves = remove_staff_lines_columnwise(staves, staff_height)\n",
    "\n",
    "# Show the processed staves\n",
    "for i, stave in enumerate(processed_staves):\n",
    "    show_images([stave], [f\"Processed Stave {i}\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segment stave into notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_staves(stave, original_stave,staff_space):    \n",
    "    \"\"\"\n",
    "    Segments the musical notes from the stave by finding contours, drawing them, \n",
    "    and extracting the notes as individual images.\n",
    "    \n",
    "    Parameters:\n",
    "        stave (np.array): Binary processed stave image.\n",
    "        original_stave (np.array): Original stave image (grayscale or binary) to extract notes.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of extracted musical note images.\n",
    "    \"\"\"\n",
    "    # Ensure the stave is in uint8 format\n",
    "    if stave.dtype != np.uint8:\n",
    "        stave = (stave * 255).astype(np.uint8)\n",
    "\n",
    "    # Find contours of the dilated stave\n",
    "    contours, _ = cv2.findContours(stave, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Sort contours left to right (by x-coordinate)\n",
    "    contours = sorted(contours, key=lambda contour: cv2.boundingRect(contour)[0])\n",
    "\n",
    "    # Create a copy of the stave to draw contours on\n",
    "    stave_with_contours = cv2.cvtColor(stave, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    # List to store extracted note images\n",
    "    notes = []\n",
    "\n",
    "    for contour in contours:\n",
    "        # Get the bounding rectangle (x, y, width, height) for each contour\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        \n",
    "        # Draw the contour as a green rectangle on the stave copy\n",
    "        if h>staff_space*2:\n",
    "            cv2.rectangle(stave_with_contours, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            \n",
    "            # Extract the note from the original stave using the bounding box\n",
    "            note = original_stave[y:y+h, x:x+w]\n",
    "            notes.append(note)\n",
    "\n",
    "    # Show the stave with contours drawn\n",
    "    show_images([stave_with_contours], [\"Stave with Contours\"])\n",
    "\n",
    "    return notes\n",
    "\n",
    "\n",
    "def dilate_vertically_with_staff_space(stave, width, height):\n",
    "    \"\"\"\n",
    "    Dilates the given stave vertically with a kernel size based on the staff space.\n",
    "    \n",
    "    Parameters:\n",
    "        stave (np.array): Binary image of the stave.\n",
    "        staff_space (int): The space between staff lines, used for vertical dilation.\n",
    "        staff_height (int): The height of the staff lines.\n",
    "        \n",
    "    Returns:\n",
    "        np.array: The vertically dilated stave.\n",
    "    \"\"\"\n",
    "    # Ensure the stave is in uint8 format\n",
    "    if stave.dtype != np.uint8:\n",
    "        stave = (stave * 255).astype(np.uint8)  # Convert boolean to uint8 (255 for white, 0 for black)\n",
    "\n",
    "    # Create a vertical structuring element (kernel)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (width, height))\n",
    "\n",
    "    # Apply vertical dilation\n",
    "    dilated_stave = cv2.dilate(stave, kernel, iterations=1)\n",
    "\n",
    "    return dilated_stave\n",
    "\n",
    "\n",
    "# Apply vertical dilation to the first processed stave\n",
    "# dilated_stave = dilate_vertically_with_staff_space(processed_staves[0], staff_space, staff_space + staff_height)\n",
    "dilated_staves=[]\n",
    "for i in range(len(processed_staves)):\n",
    "    dilated_stave = dilate_vertically_with_staff_space(processed_staves[i], staff_space, staff_space + staff_height)\n",
    "    dilated_staves.append(dilated_stave)\n",
    "\n",
    "show_images(dilated_staves)\n",
    "\n",
    "notes=[]\n",
    "for i in range(len(dilated_staves)):\n",
    "    notes.append(segment_staves(dilated_staves[i], processed_staves[i],staff_space))    \n",
    "\n",
    "for i, note in enumerate(notes):\n",
    "    show_images(note)\n",
    "# # Show the original and dilated staves\n",
    "# show_images([processed_staves[0], dilated_stave], [\"Original Processed Stave\", \"Vertically Dilated Stave\"])\n",
    "\n",
    "# Segment the staves and extract musical notes\n",
    "# notes = segment_staves(dilated_stave, processed_staves[0])\n",
    "\n",
    "# Show each extracted note\n",
    "# for i, note in enumerate(notes):\n",
    "#     show_images([note], [f\"Note {i+1}\"])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove_staff_lines_columnwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_remaining_staff_notes(note, staff_height):\n",
    "    \"\"\"\n",
    "    Removes staff lines from note by analyzing each column's run lengths.\n",
    "    If a column has a run of white pixels equal to the staff height, \n",
    "    and the above and below columns have black runs, then this column is considered a staff line and removed.\n",
    "\n",
    "    Parameters:\n",
    "        note (np.array): Binary image of the note (white = 1, black = 0).\n",
    "        staff_height (int): The detected height of the staff lines.\n",
    "\n",
    "    Returns:\n",
    "        np.array: The processed note with staff lines removed.\n",
    "    \"\"\"\n",
    "    processed_note = note.copy()\n",
    "    rows, cols = note.shape\n",
    "\n",
    "    for col in range(cols):\n",
    "        column_data = note[:, col]\n",
    "\n",
    "        bounded = np.hstack(([0], column_data, [0]))\n",
    "        difs = np.diff(bounded)\n",
    "        run_starts, = np.where(difs > 0)\n",
    "        run_ends, = np.where(difs < 0)\n",
    "\n",
    "        for i in range(len(run_starts)):\n",
    "            run_start = run_starts[i]\n",
    "            run_end = run_ends[i]\n",
    "            run_length = run_end - run_start\n",
    "\n",
    "\n",
    "            # Check if the run length is approximately equal to the staff height, the aproximation is based on the staff height\n",
    "            if run_length <= staff_height*2: # chcek in bigger images ----------------->\n",
    "                # if column_data[run_start - staff_height] == 0 and column_data[run_end +  (staff_height if (run_end + staff_height) < len(column_data))] == 0: \n",
    "                if (run_start - staff_height >= 0 and column_data[run_start - staff_height] == 0) and (run_end + staff_height < len(column_data) and column_data[run_end + staff_height] == 0):\n",
    "                    processed_note[run_start : run_end , col] = 0\n",
    "\n",
    "    return processed_note\n",
    "\n",
    "for i,note in enumerate(notes):\n",
    "    for j,notaia in enumerate(note):\n",
    "        notes[i][j] =remove_remaining_staff_notes(notaia, staff_height)\n",
    "\n",
    "for i,note in enumerate(notes):\n",
    "    show_images(note)\n",
    "# Process a sample note from the extracted notes\n",
    "# processed_note = remove_remaining_staff_notes(notes[8], staff_height)\n",
    "# # processed_note= dilate_vertically_with_staff_space(processed_note, 1, staff_height)\n",
    "\n",
    "# # Show the original and processed notes\n",
    "# show_images([notes[8], processed_note], [\"Original Note\", \"Processed Note\",])\n",
    "\n",
    "# # Process a sample note from the extracted notes\n",
    "# processed_note = remove_remaining_staff_notes(notes[18], staff_height)\n",
    "# # processed_note= dilate_vertically_with_staff_space(processed_note, 1, staff_height * 2)\n",
    "\n",
    "\n",
    "# # Show the original and processed notes\n",
    "# show_images([notes[18], processed_note,], [\"Original Note\", \"Processed Note\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 1  \n",
    "for note in notes:\n",
    "    for i in note:\n",
    "        if i.dtype == bool:\n",
    "            i = (i * 255).astype('uint8')\n",
    "        output_path = f\"{counter}.png\"\n",
    "        cv2.imwrite(output_path, i)\n",
    "        print(f\"Image saved to {output_path}\")\n",
    "        counter += 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to detect bounding boxes for staves\n",
    "\n",
    "# def getBoundedBoxes(image):\n",
    "#     #get staff bounding boxes whether they are connected or not\n",
    "#     img_height=image.shape[0]\n",
    "#     img_width=image.shape[1]\n",
    "\n",
    "#     dilation = binary_dilation(image, footprint=np.ones((10, 1)))\n",
    "#     # show_images([dilation], ['Dilation'])\n",
    "\n",
    "#     closing = remove_small_objects(dilation, 200, connectivity=8)\n",
    "#     show_images([closing], ['Closing and Small Object Removal'])\n",
    "\n",
    "\n",
    "#     img_without_begin=closing[:,int(img_height*0.2):img_height]\n",
    "#     show_images([closing,img_without_begin])\n",
    "\n",
    "#     img_without_begin = (img_without_begin * 255).astype('uint8')\n",
    "#     output_image = (image * 255).astype('uint8')\n",
    "\n",
    "\n",
    "#     contours_without_begin, _ = cv2.findContours(img_without_begin, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "#     contours_with_begin, _ = cv2.findContours(output_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "#     img_without_begin = cv2.cvtColor(img_without_begin, cv2.COLOR_GRAY2BGR)\n",
    "#     output_image = cv2.cvtColor(output_image, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "#     #check if they are connected in the begining\n",
    "#     staves=[]\n",
    "#     correct_countours=contours_with_begin\n",
    "#     is_correct_contours=False\n",
    "\n",
    "#     if len(contours_with_begin) != len(contours_without_begin):\n",
    "#         is_correct_contours=True\n",
    "#         correct_countours=contours_without_begin\n",
    "        \n",
    "#     for contour in correct_countours:\n",
    "#         x, y, w, h = cv2.boundingRect(contour)\n",
    "#         # cv2.rectangle(output_image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "#         if(w>1.1*h and w*h > 50):\n",
    "#             start_x = x\n",
    "#             end_x = img_width\n",
    "#             # print(\"width\",img_width)\n",
    "#             # print(\"final_image shape\",image.shape)\n",
    "#             if(is_correct_contours):\n",
    "#                 start_x=0\n",
    "#             stave = output_image[y:y + h, start_x:end_x]\n",
    "#             # print(\"Width\",w)\n",
    "#             # print(\"Height\",h)\n",
    "#             staves.append(stave)\n",
    "#             show_images([stave], [\"Stave222\"])\n",
    "\n",
    "#     return staves\n",
    "\n",
    "# staves2= getBoundedBoxes(final_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_staff_lines(staves):\n",
    "#     cleaned_staves = []\n",
    "#     staff_lines_positions = []\n",
    "\n",
    "#     for stave in staves:\n",
    "#         # Invert stave to make lines white on black background\n",
    "#         stave = 255 - stave\n",
    "        \n",
    "#         # Edge detection for the stave\n",
    "#         edges = cv2.Canny(stave, 50, 150, apertureSize=3)\n",
    "#         show_images([edges], ['Edges'])\n",
    "\n",
    "#         # Detect lines using Hough Transform\n",
    "#         lines = cv2.HoughLinesP(edges, 1, np.pi / 180, threshold=100, minLineLength=50, maxLineGap=10)\n",
    "        \n",
    "#         if lines is not None:\n",
    "#             line_y_positions = []\n",
    "#             for line in lines:\n",
    "#                 x1, y1, x2, y2 = line[0]\n",
    "#                 if abs(y2 - y1) < 5:  # Ensure the line is horizontal\n",
    "#                     line_y_positions.append((y1 + y2) // 2)  # Average y-coordinate for the line\n",
    "\n",
    "#             # Get 5 staff line positions by finding the most frequent y-coordinates\n",
    "#             line_y_positions = sorted(line_y_positions)\n",
    "#             staff_lines = []\n",
    "#             for y in line_y_positions:\n",
    "#                 if not staff_lines or abs(y - staff_lines[-1]) > 5:  # Avoid duplicates\n",
    "#                     staff_lines.append(y)\n",
    "#             staff_lines_positions.append(staff_lines[:5])  # Ensure only 5 lines are kept\n",
    "#         else:\n",
    "#             staff_lines_positions.append([])  # No lines detected\n",
    "\n",
    "#         stave_ratio = stave.shape[1]/stave.shape[0]\n",
    "#         print(\"Stave ratio: \"+str(stave_ratio))\n",
    "\n",
    "\n",
    "#         # Remove the detected staff lines from the stave image\n",
    "#         horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (100, 1)) # x value of MORPH_RECT range [15-100] it depend on the zoom in of ------>\n",
    "#         detected_lines = cv2.morphologyEx(stave, cv2.MORPH_OPEN, horizontal_kernel)\n",
    "        \n",
    "#         show_images([detected_lines])\n",
    "\n",
    "#         cleaned_image = stave - detected_lines\n",
    "#         cleaned_image = cleaned_image > 0\n",
    "#         dilated = binary_dilation(cleaned_image, footprint=np.ones((8, 1))) # 1st dilation depeneds on the zoom in , range [2-7] ------>\n",
    "\n",
    "#         cleaned_staves.append(dilated)\n",
    "#         # show_images([dilated], ['Cleaned Stave'])\n",
    "\n",
    "#     return cleaned_staves, staff_lines_positions\n",
    "\n",
    "# cleaned_staves, staff_lines_positions = remove_staff_lines(staves)\n",
    "# show_images(cleaned_staves)\n",
    "# print(staff_lines_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def detect_notes(cleaned_stave, staff_lines):\n",
    "#     # Step 1: Remove staff lines\n",
    "#     # stave_no_lines = cleaned_stave.copy()\n",
    "#     # for line in staff_lines:\n",
    "#     #     stave_no_lines[line - 2:line + 2, :] = 0  # Mask out staff lines (2-pixel width)\n",
    "#     # Step 2: Use morphological operations to isolate note heads\n",
    "\n",
    "#     kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (25, 25)) # it get affected by the zoom in factor,range [(3,3) - (20,20)] ------> \n",
    "#     processed_stave = cv2.morphologyEx(cleaned_stave.astype('uint8') * 255, cv2.MORPH_CLOSE, kernel)\n",
    "#     processed_stave = cv2.morphologyEx(processed_stave.astype('uint8') * 255, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "#     # Optional: Edge detection to highlight contours\n",
    "#     edges = cv2.Canny((processed_stave * 255).astype('uint8'), 50, 150)\n",
    "#     show_images([edges], ['Edges'])\n",
    "\n",
    "#     # Step 3: Find contours\n",
    "#     contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "#     show_images([cv2.drawContours(processed_stave, contours, -1, (0, 255, 0), 2)], ['Contours'])\n",
    "\n",
    "#     contours = sorted(contours, key=lambda contour: cv2.boundingRect(contour)[0])\n",
    "\n",
    "#     notes = []\n",
    "\n",
    "#     differences = [j - i for i, j in zip(staff_lines[:-1], staff_lines[1:])]\n",
    "#     average_distance_between_spaces = sum(differences) / len(differences)\n",
    "#     print(\"average dist: \" + str(average_distance_between_spaces))\n",
    "    \n",
    "#     staff_lines.append(staff_lines[-1]+int(average_distance_between_spaces))\n",
    "#     staff_lines.append(staff_lines[0]-int(average_distance_between_spaces))\n",
    "\n",
    "#     staff_lines = sorted(staff_lines)\n",
    "\n",
    "#     print(staff_lines)\n",
    "#     notes=[]\n",
    "#     for contour in contours:\n",
    "#         # Filter based on contour size (area) and aspect ratio\n",
    "#         area = cv2.contourArea(contour)\n",
    "#         x, y, w, h = cv2.boundingRect(contour)\n",
    "#         aspect_ratio = w / h if h > 0 else 0\n",
    "\n",
    "#         factor = 12 # this factor is for the value of zoom in\n",
    "#         print(\"area: \" + str(area) + \", aspect ratio: \" + str(aspect_ratio))\n",
    "#         if (10 * factor) < area < (300 * factor) and 0.7 < aspect_ratio < 1.3:  # Typical note head properties  area varies with the zoom in, range [300] --------->\n",
    "#             note_center = y + h // 2  # Vertical center of the note head\n",
    "\n",
    "#             print(\"note center: \" + str(note_center) + \",x= \" + str(x) + \",y1= \" + str(y) +\",y2= \" + str(y+h))\n",
    "\n",
    "#             note_names_treble = ['C','D','E','F','G','A','B','C2','D2','E2','F2','G2','A2','B2']\n",
    "#             y1=y + int(average_distance_between_spaces/ 8)\n",
    "#             y2=y+h - int(average_distance_between_spaces/ 8)\n",
    "\n",
    "#             if  y2>staff_lines[-1] and y1<staff_lines[-1]:\n",
    "#                 notes.append('C')\n",
    "#             elif y2<=staff_lines[-1] and y1>=staff_lines[-2]:\n",
    "#                 notes.append('D')\n",
    "#             elif y2>staff_lines[-2] and y1<staff_lines[-2]:\n",
    "#                 notes.append('E')\n",
    "#             elif y2<=staff_lines[-2] and y1>=staff_lines[-3]:\n",
    "#                 notes.append('F')\n",
    "#             elif y2>staff_lines[-3] and y1<staff_lines[-3]:\n",
    "#                 notes.append('G')\n",
    "#             elif y2<=staff_lines[-3] and y1>=staff_lines[-4]:\n",
    "#                 notes.append('A')\n",
    "#             elif y2>staff_lines[-4] and y1<staff_lines[-4]:\n",
    "#                 notes.append('B')\n",
    "#             elif y2<=staff_lines[-4] and y1>=staff_lines[-5]:\n",
    "#                 notes.append('C2')\n",
    "#             elif y2>staff_lines[-5] and y1<staff_lines[-5]:\n",
    "#                 notes.append('D2')\n",
    "#             elif y2<=staff_lines[-5] and y1>=staff_lines[-6]:\n",
    "#                 notes.append('E2')\n",
    "#             elif y2>staff_lines[-6] and y1<staff_lines[-6]:\n",
    "#                 notes.append('F2')\n",
    "#             elif y2<=staff_lines[-6] and y1>=staff_lines[-7]:\n",
    "#                 notes.append('A2')\n",
    "#             elif y2>staff_lines[-7] and y1<staff_lines[-7]:\n",
    "#                 notes.append('B2')\n",
    "\n",
    "    \n",
    "#     return notes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to map note positions to musical notes\n",
    "# def map_positions_to_notes(positions, clef='treble'):\n",
    "#     note_names_treble = ['F', 'E', 'D', 'C', 'B', 'A', 'G']\n",
    "#     note_names_bass = ['A', 'G', 'F', 'E', 'D', 'C', 'B']\n",
    "    \n",
    "#     note_names = note_names_treble if clef == 'treble' else note_names_bass\n",
    "#     mapped_notes = [note_names[pos % len(note_names)] for pos in positions if pos >= 0]\n",
    "#     return mapped_notes\n",
    "\n",
    "# # Main processing\n",
    "# # final_image = cv2.imread('sheet_music.png', cv2.IMREAD_GRAYSCALE)  # Load your image here\n",
    "# # final_image = final_image < 128  # Binarize the image\n",
    "# # show_images([final_image], [\"Final Image\"])\n",
    "\n",
    "# # staves = getBoundedBoxes(final_image)\n",
    "# # cleaned_staves, staff_lines_positions = remove_staff_lines(staves)\n",
    "\n",
    "# print(staff_lines_positions)\n",
    "\n",
    "# for i, (stave, staff_lines) in enumerate(zip(cleaned_staves, staff_lines_positions)):\n",
    "#     notes = detect_notes(stave, staff_lines)\n",
    "#     print(f\"Detected Notes in Stave {i + 1}: {notes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
