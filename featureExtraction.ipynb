{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from sklearn.utils import resample\n",
    "from matplotlib.pyplot import bar\n",
    "from skimage.exposure import histogram\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Show the figures / plots inside the notebook\n",
    "def show_images(images,titles=None):\n",
    "    #This function is used to show image(s) with titles by sending an array of images and an array of associated titles.\n",
    "    # images[0] will be drawn with the title titles[0] if exists\n",
    "    # You aren't required to understand this function, use it as-is.\n",
    "    n_ims = len(images)\n",
    "    if titles is None: titles = ['(%d)' % i for i in range(1,n_ims + 1)]\n",
    "    fig = plt.figure()\n",
    "    n = 1\n",
    "    for image,title in zip(images,titles):\n",
    "        a = fig.add_subplot(1,n_ims,n)\n",
    "        if image.ndim == 2: \n",
    "            plt.gray()\n",
    "        plt.imshow(image)\n",
    "        a.set_title(title)\n",
    "        n += 1\n",
    "    fig.set_size_inches(np.array(fig.get_size_inches()) * n_ims)\n",
    "    plt.show() \n",
    "\n",
    "\n",
    "def showHist(img):\n",
    "    # An \"interface\" to matplotlib.axes.Axes.hist() method\n",
    "    plt.figure()\n",
    "    imgHist = histogram(img, nbins=256)\n",
    "    \n",
    "    bar(imgHist[1].astype(np.uint8), imgHist[0], width=0.8, align='center')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_and_convert_images(folder_path):\n",
    "    # Get all files in the folder\n",
    "    files = os.listdir(folder_path)\n",
    "    \n",
    "    # Filter image files and sort them\n",
    "    image_files = [f for f in files if f.lower().endswith(('.png', '.bmp', '.jpg', '.jpeg'))]\n",
    "    image_files.sort()  # Optional: Ensures renaming follows sorted order\n",
    "\n",
    "    # Process each image\n",
    "    for idx, file_name in enumerate(image_files, start=1):\n",
    "        old_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        # Set new name with index and PNG extension\n",
    "        new_name = f\"{idx}.png\"\n",
    "        new_path = os.path.join(folder_path, new_name)\n",
    "        \n",
    "        try:\n",
    "            # Convert BMP to PNG and resize\n",
    "            img = cv2.imread(old_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if img is None:\n",
    "                print(f\"Failed to load image: {file_name}\")\n",
    "                continue\n",
    "            \n",
    "            # background_pixel = img[0, 0]\n",
    "            # if background_pixel < 128:  # Background is not white (dark)\n",
    "            #     # print(f\"Inverting background for {file_name}\")\n",
    "            #     img = cv2.bitwise_not(img)  # Invert the colors\n",
    "\n",
    "            # Resize to 32x32\n",
    "            resized_img = cv2.resize(img, (32, 32))\n",
    "            \n",
    "            # Save the resized image using PIL for PNG format\n",
    "            pil_img = Image.fromarray(resized_img)\n",
    "            pil_img.save(new_path, \"PNG\")\n",
    "            \n",
    "            # Remove the old BMP file\n",
    "            os.remove(old_path)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {e}\")\n",
    "            continue  # Skip to the next file if an error occurs\n",
    "\n",
    "    print(\"Renaming, resizing, and conversion completed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hog_features(img):\n",
    "    # Ensure input is a NumPy array\n",
    "    if img is None or not isinstance(img, np.ndarray):\n",
    "        raise ValueError(\"Invalid input image. Ensure it's loaded correctly.\")\n",
    "    # Resize image to target size\n",
    "    img = cv2.resize(img, (32, 32))\n",
    "    show_images([img])\n",
    "    \n",
    "    # Define HOG descriptor parameters\n",
    "    win_size = (32, 32)\n",
    "    cell_size = (4, 4)\n",
    "    block_size_in_cells = (2, 2)\n",
    "    block_size = (block_size_in_cells[1] * cell_size[1],\n",
    "                  block_size_in_cells[0] * cell_size[0])\n",
    "    block_stride = (cell_size[1], cell_size[0])\n",
    "    nbins = 9\n",
    "    \n",
    "    # Initialize HOG descriptor\n",
    "    hog = cv2.HOGDescriptor(win_size, block_size, block_stride, cell_size, nbins)\n",
    "    \n",
    "    # Compute HOG features\n",
    "    h = hog.compute(img)\n",
    "    return h.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images_in_folder(label_folder, label, output_csv=\"features_with_labels.csv\"):\n",
    "    # Check if the CSV file exists\n",
    "    file_exists = os.path.isfile(output_csv)\n",
    "    \n",
    "    with open(output_csv, \"a\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        # Write header if the file doesn't exist\n",
    "        if not file_exists:\n",
    "            num_features = 1764  # Adjust to match your HOG settings\n",
    "            header = [f\"x{i+1}\" for i in range(num_features)] + [\"y\"]\n",
    "            writer.writerow(header)\n",
    "            print(f\"Created {output_csv} and added header.\")\n",
    "        \n",
    "        # Iterate through all image files in the label folder\n",
    "        for file_name in os.listdir(label_folder):\n",
    "            file_path = os.path.join(label_folder, file_name)\n",
    "            \n",
    "            # Skip non-image files\n",
    "            if not file_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "                continue\n",
    "            \n",
    "            # Load the image\n",
    "            img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)  # Use grayscale for HOG\n",
    "            if img is None:\n",
    "                print(f\"Failed to load image: {file_name}\")\n",
    "                continue\n",
    "            \n",
    "            # Extract HOG features\n",
    "            features = extract_hog_features(img)\n",
    "            print(features)\n",
    "            # Append the label and write to the CSV\n",
    "            writer.writerow(np.append(features, label))\n",
    "            print(f\"Processed: {file_name} with label {label}\")\n",
    "\n",
    "    print(f\"Features and labels saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Provide the folder path containing the images\n",
    "# folder_path = \"./dataset/flat\"\n",
    "# rename_and_convert_images(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# label_folder = \"./dataset/a_1\"\n",
    "# process_images_in_folder(label_folder,\"11\",\"./csvs/a_1_features_with_labels_11.csv\")\n",
    "\n",
    "# label_folder = \"./dataset/a_2\"\n",
    "# process_images_in_folder(label_folder,\"12\",\"./csvs/a_2_features_with_labels_12.csv\")\n",
    "\n",
    "# label_folder = \"./dataset/a_4\"\n",
    "# process_images_in_folder(label_folder,\"14\",\"./csvs/a_4_features_with_labels_14.csv\")\n",
    "\n",
    "# label_folder = \"./dataset/a_8\"\n",
    "# process_images_in_folder(label_folder,\"18\",\"./csvs/a_8_features_with_labels_18.csv\")\n",
    "\n",
    "# label_folder = \"./dataset/a_16\"\n",
    "# process_images_in_folder(label_folder,\"16\",\"./csvs/a_16_features_with_labels_16.csv\")\n",
    "\n",
    "# label_folder = \"./dataset/a_32\"\n",
    "# process_images_in_folder(label_folder,\"32\",\"./csvs/a_32_features_with_labels_32.csv\")\n",
    "\n",
    "# label_folder = \"./dataset/double_flat\"\n",
    "# process_images_in_folder(label_folder,\"33\",\"./csvs/double_flat_features_with_labels_33.csv\")\n",
    "\n",
    "# label_folder = \"./dataset/double_sharp\"\n",
    "# process_images_in_folder(label_folder,\"34\",\"./csvs/double_sharp_features_with_labels_34.csv\")\n",
    "\n",
    "# label_folder = \"./dataset/flat\"\n",
    "# process_images_in_folder(label_folder,\"35\",\"./csvs/flat_features_with_labels_35.csv\")\n",
    "\n",
    "# label_folder = \"./dataset/natural\"\n",
    "# process_images_in_folder(label_folder,\"36\",\"./csvs/natural_features_with_labels_36.csv\")\n",
    "\n",
    "# label_folder = \"./dataset/sharp\"\n",
    "# process_images_in_folder(label_folder,\"37\",\"./csvs/sharp_features_with_labels_37.csv\")\n",
    "\n",
    "# label_folder = \"./dataset/digits_dataset/1\"\n",
    "# process_images_in_folder(label_folder,\"1\",\"./csvs/1_features_with_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Load All CSV Files from a Folder\n",
    "def load_csvs_from_folder(folder_path):\n",
    "    csv_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "    dataframes = [pd.read_csv(csv_file) for csv_file in csv_files]\n",
    "    return dataframes\n",
    "\n",
    "# Step 2: Balance Each Dataset to Target Size\n",
    "def balance_to_avg(dataframes, target_size=500):\n",
    "    balanced_dataframes = []\n",
    "    for df in dataframes:\n",
    "        if len(df) < target_size:\n",
    "            # Augment smaller datasets by resampling\n",
    "            augmented_df = resample(\n",
    "                df, \n",
    "                replace=True, \n",
    "                n_samples=target_size, \n",
    "                random_state=42\n",
    "            )\n",
    "            balanced_dataframes.append(augmented_df)\n",
    "        else:\n",
    "            # Downsample larger datasets\n",
    "            downsampled_df = df.sample(n=target_size, random_state=42)\n",
    "            balanced_dataframes.append(downsampled_df)\n",
    "    return pd.concat(balanced_dataframes)\n",
    "\n",
    "# Step 3: Train and Evaluate Classifiers\n",
    "def train_classifiers(data):\n",
    "    # Separate features and labels\n",
    "    X = data.iloc[:, :-1].values  # All columns except the last one\n",
    "    y = data.iloc[:, -1].values   # Last column as target\n",
    "    \n",
    "    # Convert labels to numeric if necessary\n",
    "    if y.dtype == 'object':\n",
    "        y = y.astype(int)\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    # Split into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    print(\"X_train\")\n",
    "    print(X_train)\n",
    "    # Define classifiers\n",
    "    # svc = SVC(kernel='linear', probability=True, random_state=42)\n",
    "    svc = SVC(kernel='poly',degree=2,C=0.1,gamma=0.1,probability=True,random_state=42) \n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "    log_reg = LogisticRegression(random_state=42, max_iter=1000)  # Added Logistic Regression\n",
    "\n",
    "    # Voting classifier\n",
    "    voting_clf = VotingClassifier(\n",
    "        estimators=[('SVM', svc), ('RF', rf)],\n",
    "        voting='soft'\n",
    "    )\n",
    "\n",
    "    # Train classifiers\n",
    "    voting_clf.fit(X_train, y_train)\n",
    "    svc.fit(X_train, y_train)\n",
    "    rf.fit(X_train, y_train)\n",
    "    log_reg.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate classifiers\n",
    "    voting_preds = voting_clf.predict(X_test)\n",
    "    svc_preds = svc.predict(X_test)\n",
    "    rf_preds = rf.predict(X_test)\n",
    "    log_reg_preds = log_reg.predict(X_test)\n",
    "\n",
    "    # Print accuracies\n",
    "    print(\"Voting Classifier Accuracy:\", accuracy_score(y_test, voting_preds))\n",
    "    print(\"SVM Accuracy:\", accuracy_score(y_test, svc_preds))\n",
    "    print(\"Random Forest Accuracy:\", accuracy_score(y_test, rf_preds))\n",
    "    print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, log_reg_preds))\n",
    "\n",
    "    return voting_clf,svc,rf,log_reg,scaler\n",
    "\n",
    "# Specify the folder containing your CSV files\n",
    "folder_path = \"./csvs\"  # Replace with your folder path if different\n",
    "\n",
    "# Load, balance, and combine the datasets\n",
    "dataframes = load_csvs_from_folder(folder_path)\n",
    "balanced_data = balance_to_avg(dataframes, target_size=500)\n",
    "\n",
    "# Train and evaluate classifiers\n",
    "voting_clf,svc,rf,log_reg,scaler=train_classifiers(balanced_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define classifiers\n",
    "clf1 = LinearDiscriminantAnalysis()\n",
    "clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "clf4= SVC(kernel='poly',degree=2,C=0.1,gamma=0.1) \n",
    "\n",
    "# Specify the folder containing your CSV files\n",
    "folder_path = \"./csvs\"  # Replace with your folder path if different\n",
    "\n",
    "# Load, balance, and combine the datasets\n",
    "dataframes = load_csvs_from_folder(folder_path)\n",
    "balanced_data = balance_to_avg(dataframes, target_size=500)\n",
    "classifiers = [('LDA', clf1), ('RF', clf2), ('GNB', clf3),('SVC', clf4)]\n",
    "X = balanced_data.iloc[:, :-1].values  # All columns except the last one\n",
    "y = balanced_data.iloc[:, -1].values   # Last column as target\n",
    "\n",
    "# Convert labels to numeric if necessary\n",
    "if y.dtype == 'object':\n",
    "    y = y.astype(int)\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# 3. Train and evaluate each classifier individually using cross-validation\n",
    "for name, clf in classifiers:\n",
    "    scores = cross_val_score(clf, X, y, cv=5)  \n",
    "    print(f\"{name} Accuracy: {np.mean(scores):.4f}\")\n",
    "\n",
    "# 4. Hard voting classifier\n",
    "eclf1 = VotingClassifier(estimators=classifiers, voting='hard')\n",
    "\n",
    "# 5. Evaluate the hard voting classifier using cross-validation\n",
    "scores = cross_val_score(eclf1, X, y, cv=5)  \n",
    "print(f\"Hard Voting Classifier Accuracy: {np.mean(scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Split data into test and validation sets\n",
    "X_train_new, X_val, y_train_new, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "# Step 2: Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'degree': [1,2],  # Test different polynomial degrees\n",
    "    'C': [0.01,0.1, 1],  # Test different values for regularization parameter C\n",
    "    'gamma': [0.01,0.1, 1]  # Test different gamma values\n",
    "}\n",
    "\n",
    "# Step 3: Create the SVM model\n",
    "svm_model = SVC(kernel='poly')\n",
    "\n",
    "# Step 4: Use GridSearchCV for parameter tuning\n",
    "grid_search = GridSearchCV(svm_model, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_val, y_val)\n",
    "\n",
    "# Step 5: Get the best parameters and retrain the model\n",
    "best_params = grid_search.best_params_\n",
    "best_svm_model = SVC(kernel='poly', **best_params)\n",
    "best_svm_model.fit(X_train, y_train)  # Retrain on the entire X_train\n",
    "\n",
    "# Step 6: Evaluate the final model on the test set\n",
    "test_accuracy = best_svm_model.score(X_test, y_test)\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Test Accuracy:\",test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save models and scaler to .pkl files\n",
    "with open('./models/voting_clf.pkl', 'wb') as f:\n",
    "    pickle.dump(voting_clf, f)\n",
    "\n",
    "with open('./models/svc.pkl', 'wb') as f:\n",
    "    pickle.dump(svc, f)\n",
    "\n",
    "with open('./models/rf.pkl', 'wb') as f:\n",
    "    pickle.dump(rf, f)\n",
    "\n",
    "with open('./models/log_reg.pkl', 'wb') as f:\n",
    "    pickle.dump(log_reg, f)\n",
    "\n",
    "with open('./models/scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(\"Models and scaler saved successfully using pickle!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models and scaler from .pkl files\n",
    "with open('./models/voting_clf.pkl', 'rb') as f:\n",
    "    voting_clf = pickle.load(f)\n",
    "\n",
    "with open('./models/svc.pkl', 'rb') as f:\n",
    "    svc = pickle.load(f)\n",
    "\n",
    "with open('./models/rf.pkl', 'rb') as f:\n",
    "    rf = pickle.load(f)\n",
    "\n",
    "with open('./models/log_reg.pkl', 'rb') as f:\n",
    "    log_reg = pickle.load(f)\n",
    "\n",
    "with open('./models/scaler.pkl', 'rb') as f:\n",
    "    scaler = pickle.load(f)\n",
    "\n",
    "print(\"Models and scaler loaded successfully using pickle!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a new input image\n",
    "def predict_image(image_path, voting_clf, svc, rf, log_reg, scaler):\n",
    "     # Load the image\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Use grayscale for HOG\n",
    "    if img is None:\n",
    "        print(f\"Failed to load image: {image_path}\")\n",
    "\n",
    "    # Convert the grayscale image to binary using thresholding\n",
    "    _, binary_img = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Check if the background (top-left pixel) is dark\n",
    "    background_pixel = binary_img[0, 0]\n",
    "    if background_pixel == 0:  # Background is dark\n",
    "        print(f\"Inverting binary image\")\n",
    "        binary_img = cv2.bitwise_not(binary_img)  # Invert binary image  \n",
    "\n",
    "    show_images([binary_img])\n",
    "    # Preprocess the image and extract features\n",
    "    features = extract_hog_features(img)\n",
    "\n",
    "    # Standardize the features using the same scaler as during training\n",
    "    features = scaler.transform([features])\n",
    "\n",
    "    # Predict using each classifier\n",
    "    voting_prediction = voting_clf.predict(features)[0]\n",
    "    svc_prediction = svc.predict(features)[0]\n",
    "    rf_prediction = rf.predict(features)[0]\n",
    "    log_reg_prediction = log_reg.predict(features)[0]\n",
    "\n",
    "    return {\n",
    "        \"Voting Classifier\": voting_prediction,\n",
    "        \"SVM\": svc_prediction,\n",
    "        \"Random Forest\": rf_prediction,\n",
    "        \"Logisitc Regression\": log_reg_prediction,\n",
    "    }\n",
    "\n",
    "image_path = \"image2.png\"  # Replace with your test image path\n",
    "predictions = predict_image(image_path, voting_clf, svc, rf,log_reg, scaler)\n",
    "\n",
    "print(\"\\nPredictions for the input image:\")\n",
    "for classifier, prediction in predictions.items():\n",
    "    print(f\"{classifier}: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a mapping for predictions\n",
    "label_mapping = {\n",
    "    1: \"digit 1\",\n",
    "    2: \"digit 2\",\n",
    "    3: \"digit 3\",\n",
    "    4: \"digit 4\",\n",
    "    5: \"digit 5\",\n",
    "    6: \"digit 6\",\n",
    "    7: \"digit 7\",\n",
    "    8: \"digit 8\",\n",
    "    9: \"digit 9\",\n",
    "    11: \"a_1\",\n",
    "    12: \"a_2\",\n",
    "    14: \"a_4\",\n",
    "    18: \"a_8\",\n",
    "    16: \"a_16\",\n",
    "    32: \"a_32\",\n",
    "    33: \"double flat\",\n",
    "    34: \"double sharp\",\n",
    "    35: \"flat\",\n",
    "    36: \"natural\",\n",
    "    37: \"sharp\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
