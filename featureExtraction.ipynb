{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from sklearn.utils import resample\n",
    "from matplotlib.pyplot import bar\n",
    "from skimage.exposure import histogram\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Show the figures / plots inside the notebook\n",
    "def show_images(images,titles=None):\n",
    "    #This function is used to show image(s) with titles by sending an array of images and an array of associated titles.\n",
    "    # images[0] will be drawn with the title titles[0] if exists\n",
    "    # You aren't required to understand this function, use it as-is.\n",
    "    n_ims = len(images)\n",
    "    if titles is None: titles = ['(%d)' % i for i in range(1,n_ims + 1)]\n",
    "    fig = plt.figure()\n",
    "    n = 1\n",
    "    for image,title in zip(images,titles):\n",
    "        a = fig.add_subplot(1,n_ims,n)\n",
    "        if image.ndim == 2: \n",
    "            plt.gray()\n",
    "        plt.imshow(image)\n",
    "        a.set_title(title)\n",
    "        n += 1\n",
    "    fig.set_size_inches(np.array(fig.get_size_inches()) * n_ims)\n",
    "    plt.show() \n",
    "\n",
    "\n",
    "def showHist(img):\n",
    "    # An \"interface\" to matplotlib.axes.Axes.hist() method\n",
    "    plt.figure()\n",
    "    imgHist = histogram(img, nbins=256)\n",
    "    \n",
    "    bar(imgHist[1].astype(np.uint8), imgHist[0], width=0.8, align='center')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(img): \n",
    "    # Resize image to target size\n",
    "    # Get original dimensions\n",
    "    original_height, original_width = img.shape[:2]\n",
    "    target_width, target_height = 32 , 32\n",
    "# \n",
    "    # Calculate scaling factor while maintaining aspect ratio\n",
    "    scale = min(target_width / original_width, target_height / original_height)\n",
    "\n",
    "    # Calculate new dimensions after scaling\n",
    "    new_width = int(original_width * scale)\n",
    "    new_height = int(original_height * scale)\n",
    "\n",
    "    # Resize the image\n",
    "    resized_img = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # Calculate padding to center the resized image\n",
    "    pad_top = (target_height - new_height) // 2\n",
    "    pad_bottom = target_height - new_height - pad_top\n",
    "    pad_left = (target_width - new_width) // 2\n",
    "    pad_right = target_width - new_width - pad_left\n",
    "\n",
    "    # Pad the resized image with white (255)\n",
    "    padded_img = cv2.copyMakeBorder(\n",
    "        resized_img,\n",
    "        pad_top, pad_bottom, pad_left, pad_right,\n",
    "        borderType=cv2.BORDER_CONSTANT,\n",
    "        value=[255, 255, 255]  # White background\n",
    "    )\n",
    "    \n",
    "    # padded_img[padded_img<200]=0\n",
    "    # padded_img[padded_img>=200]=255\n",
    "    # show_images([padded_img],[\"padded imag\"])\n",
    "\n",
    "    return padded_img\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_and_convert_images(folder_path):\n",
    "    # Get all files in the folder\n",
    "    files = os.listdir(folder_path)\n",
    "    \n",
    "    # Filter image files and sort them\n",
    "    image_files = [f for f in files if f.lower().endswith(('.png', '.bmp', '.jpg', '.jpeg'))]\n",
    "    image_files.sort()  # Optional: Ensures renaming follows sorted order\n",
    "\n",
    "    # Process each image\n",
    "    for idx, file_name in enumerate(image_files, start=1):\n",
    "        old_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        # Set new name with index and PNG extension\n",
    "        new_name = f\"{idx}.png\"\n",
    "        new_path = os.path.join(folder_path, new_name)\n",
    "        \n",
    "        try:\n",
    "            # Convert BMP to PNG and resize\n",
    "            img = cv2.imread(old_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if img is None:\n",
    "                print(f\"Failed to load image: {file_name}\")\n",
    "                continue\n",
    "            \n",
    "            # background_pixel = img[0, 0]\n",
    "            # if background_pixel < 128:  # Background is not white (dark)\n",
    "            #     # print(f\"Inverting background for {file_name}\")\n",
    "            #     img = cv2.bitwise_not(img)  # Invert the colors\n",
    "\n",
    "            # Resize to 32x32\n",
    "            resized_img = resize_image(img)\n",
    "            \n",
    "            # Save the resized image using PIL for PNG format\n",
    "            pil_img = Image.fromarray(resized_img)\n",
    "            pil_img.save(new_path, \"PNG\")\n",
    "            \n",
    "            # Remove the old BMP file\n",
    "            os.remove(old_path)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {e}\")\n",
    "            continue  # Skip to the next file if an error occurs\n",
    "\n",
    "    print(\"Renaming, resizing, and conversion completed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hog_features(img):\n",
    "    # Ensure input is a NumPy array\n",
    "    if img is None or not isinstance(img, np.ndarray):\n",
    "        raise ValueError(\"Invalid input image. Ensure it's loaded correctly.\")\n",
    "   \n",
    "    # Define HOG descriptor parameters\n",
    "    win_size = (32, 32)\n",
    "    cell_size = (4, 4)\n",
    "    block_size_in_cells = (2, 2)\n",
    "    block_size = (block_size_in_cells[1] * cell_size[1],\n",
    "                  block_size_in_cells[0] * cell_size[0])\n",
    "    block_stride = (cell_size[1], cell_size[0])\n",
    "    nbins = 9\n",
    "    \n",
    "    # Initialize HOG descriptor\n",
    "    hog = cv2.HOGDescriptor(win_size, block_size, block_stride, cell_size, nbins)\n",
    "    \n",
    "    # Compute HOG features\n",
    "    h = hog.compute(img)\n",
    "    return h.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_keypoints_and_descriptors(img):\n",
    "    \"\"\"\n",
    "    Extract SIFT keypoints and descriptors from an image.\n",
    "    \"\"\"\n",
    "    if img is None or not isinstance(img, np.ndarray):\n",
    "        raise ValueError(\"Invalid input image. Ensure it's loaded correctly.\")\n",
    "    \n",
    "\n",
    "    # Create a SIFT detector and extract descriptors\n",
    "    sift = cv2.SIFT_create()\n",
    "    keypoints, descriptors = sift.detectAndCompute(padded_img, None)\n",
    "\n",
    "    if descriptors is None:\n",
    "        descriptors = np.array([])  # Handle empty descriptors\n",
    "\n",
    "    return keypoints, descriptors\n",
    "\n",
    "def compute_bow_histogram(descriptors, kmeans, k):\n",
    "    \"\"\"\n",
    "    Compute a Bag of Words histogram for a single image.\n",
    "    \"\"\"\n",
    "    histogram = np.zeros(k)\n",
    "    if descriptors.size > 0:  # Ensure descriptors are not empty\n",
    "        cluster_assignments = kmeans.predict(descriptors)\n",
    "        for cluster in cluster_assignments:\n",
    "            histogram[cluster] += 1\n",
    "    return histogram\n",
    "\n",
    "def process_images_to_sift_bow(label_folder, output_csv, k=100):\n",
    "    \"\"\"\n",
    "    Process images in a folder, extract SIFT Bag of Words features,\n",
    "    and save the features into a CSV file.\n",
    "    \"\"\"\n",
    "    all_descriptors = []\n",
    "\n",
    "    # Step 1: Extract SIFT descriptors from all images\n",
    "    for file_name in os.listdir(label_folder):\n",
    "        file_path = os.path.join(label_folder, file_name)\n",
    "        img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is None:\n",
    "            print(f\"Failed to load image: {file_name}\")\n",
    "            continue\n",
    "\n",
    "        _, descriptors = extract_keypoints_and_descriptors(img)\n",
    "        if descriptors.size > 0:\n",
    "            all_descriptors.append(descriptors)\n",
    "\n",
    "    if len(all_descriptors) == 0:\n",
    "        print(\"No descriptors extracted from images.\")\n",
    "        return\n",
    "\n",
    "    all_descriptors = np.vstack(all_descriptors)  # Combine all descriptors\n",
    "\n",
    "    # Step 2: Perform K-Means clustering to create visual words\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0).fit(all_descriptors)\n",
    "\n",
    "    # Step 3: Compute BoW histograms for each image\n",
    "    features = []\n",
    "    for file_name in os.listdir(label_folder):\n",
    "        file_path = os.path.join(label_folder, file_name)\n",
    "        img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is None:\n",
    "            continue\n",
    "\n",
    "        _, descriptors = extract_keypoints_and_descriptors(img)\n",
    "        histogram = compute_bow_histogram(descriptors, kmeans, k)\n",
    "        features.append(histogram)\n",
    "\n",
    "    # Step 4: Save features to a CSV file\n",
    "    with open(output_csv, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([f\"Feature_{i+1}\" for i in range(k)] + [\"Label\"])  # Add header\n",
    "        for histogram in features:\n",
    "            writer.writerow(list(histogram))  # Save histogram\n",
    "\n",
    "    print(f\"SIFT Bag of Words features saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images_in_folder(label_folder, label, output_csv=\"features_with_labels.csv\"):\n",
    "    # Check if the CSV file exists\n",
    "    file_exists = os.path.isfile(output_csv)\n",
    "    \n",
    "    with open(output_csv, \"a\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        # Write header if the file doesn't exist\n",
    "        if not file_exists:\n",
    "            num_features = 1764  # Adjust to match your HOG settings\n",
    "            header = [f\"x{i+1}\" for i in range(num_features)] + [\"y\"]\n",
    "            writer.writerow(header)\n",
    "            print(f\"Created {output_csv} and added header.\")\n",
    "        \n",
    "        # Iterate through all image files in the label folder\n",
    "        for file_name in os.listdir(label_folder):\n",
    "            file_path = os.path.join(label_folder, file_name)\n",
    "            \n",
    "            # # Skip non-image files\n",
    "            # if not file_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "            #     continue\n",
    "            \n",
    "            # Load the image\n",
    "            img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)  # Use grayscale for HOG\n",
    "            if img is None:\n",
    "                print(f\"Failed to load image: {file_name}\")\n",
    "                continue\n",
    "            \n",
    "            # Extract HOG features\n",
    "            features = extract_hog_features(img)\n",
    "            # print(features)\n",
    "            # Append the label and write to the CSV\n",
    "            writer.writerow(np.append(features, label))\n",
    "            print(f\"Processed: {file_name} with label {label}\")\n",
    "\n",
    "    print(f\"Features and labels saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Provide the folder path containing the images\n",
    "folder_path = \"./dataset/b_16\"\n",
    "rename_and_convert_images(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# label_folder = \"./dataset/a_1\"\n",
    "# process_images_in_folder(label_folder,\"11\",\"./csvs/a_1_features_with_labels_11.csv\")\n",
    "\n",
    "# label_folder = \"./dataset/a_2\"\n",
    "# process_images_in_folder(label_folder,\"12\",\"./csvs/a_2_features_with_labels_12.csv\")\n",
    "\n",
    "# label_folder = \"./dataset/a_4\"\n",
    "# process_images_in_folder(label_folder,\"14\",\"./csvs/a_4_features_with_labels_14.csv\")\n",
    "\n",
    "# label_folder = \"./dataset/a_8\"\n",
    "# process_images_in_folder(label_folder,\"18\",\"./csvs/a_8_features_with_labels_18.csv\")\n",
    "\n",
    "# label_folder = \"./dataset/a_16\"\n",
    "# process_images_in_folder(label_folder,\"16\",\"./csvs/a_16_features_with_labels_16.csv\")\n",
    "\n",
    "# label_folder = \"./dataset/a_32\"\n",
    "# process_images_in_folder(label_folder,\"32\",\"./csvs/a_32_features_with_labels_32.csv\")\n",
    "\n",
    "# label_folder = \"./dataset/double_flat\"\n",
    "# process_images_in_folder(label_folder,\"33\",\"./csvs/double_flat_features_with_labels_33.csv\")\n",
    "\n",
    "# label_folder = \"./dataset/double_sharp\"\n",
    "# process_images_in_folder(label_folder,\"34\",\"./csvs/double_sharp_features_with_labels_34.csv\")\n",
    "\n",
    "# label_folder = \"./dataset/flat\"\n",
    "# process_images_in_folder(label_folder,\"35\",\"./csvs/flat_features_with_labels_35.csv\")\n",
    "\n",
    "# label_folder = \"./dataset/natural\"\n",
    "# process_images_in_folder(label_folder,\"36\",\"./csvs/natural_features_with_labels_36.csv\")\n",
    "\n",
    "# label_folder = \"./dataset/sharp\"\n",
    "# process_images_in_folder(label_folder,\"37\",\"./csvs/sharp_features_with_labels_37.csv\")\n",
    "\n",
    "# label_folder = \"./dataset/digits_dataset/1\"\n",
    "# process_images_in_folder(label_folder,\"1\",\"./csvs/1_features_with_labels.csv\")\n",
    "\n",
    "\n",
    "# label_folder = \"./dataset/b_2\"\n",
    "# process_images_in_folder(label_folder,\"22\",\"./csvs/b_2_features_with_labels_22.csv\")\n",
    "# label_folder = \"./dataset/b_4\"\n",
    "# process_images_in_folder(label_folder,\"24\",\"./csvs/b_4_features_with_labels_24.csv\")\n",
    "# label_folder = \"./dataset/b_8\"\n",
    "# process_images_in_folder(label_folder,\"28\",\"./csvs/b_8_features_with_labels_28.csv\")\n",
    "label_folder = \"./dataset/b_16\"\n",
    "process_images_in_folder(label_folder,\"26\",\"./csvs/b_16_features_with_labels_26.csv\")\n",
    "# label_folder = \"./dataset/b_32\"\n",
    "# process_images_in_folder(label_folder,\"232\",\"./csvs/b_32_features_with_labels_232.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Load All CSV Files from a Folder\n",
    "def load_csvs_from_folder(folder_path):\n",
    "    csv_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "    dataframes = [pd.read_csv(csv_file) for csv_file in csv_files]\n",
    "    return  pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Step 2: Balance Each Dataset to Target Size\n",
    "# def balance_to_avg(dataframes, target_size=500):\n",
    "#     balanced_dataframes = []\n",
    "#     for df in dataframes:\n",
    "#         if len(df) < target_size:\n",
    "#             # Augment smaller datasets by resampling\n",
    "#             augmented_df = resample(\n",
    "#                 df, \n",
    "#                 replace=True, \n",
    "#                 n_samples=target_size, \n",
    "#                 random_state=42\n",
    "#             )\n",
    "#             balanced_dataframes.append(augmented_df)\n",
    "#         else:\n",
    "#             # Downsample larger datasets\n",
    "#             downsampled_df = df.sample(n=target_size, random_state=42)\n",
    "#             balanced_dataframes.append(downsampled_df)\n",
    "#     return pd.concat(balanced_dataframes)\n",
    "\n",
    "# Step 3: Train and Evaluate Classifiers\n",
    "def train_classifiers(data):\n",
    "    # Separate features and labels\n",
    "    X = data.iloc[:, :-1].values  # All columns except the last one\n",
    "    y = data.iloc[:, -1].values   # Last column as target\n",
    "    \n",
    "    # Convert labels to numeric if necessary\n",
    "    if y.dtype == 'object':\n",
    "        y = y.astype(int)\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    # Split into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    print(\"X_train\")\n",
    "    print(X_train)\n",
    "    # Define classifiers\n",
    "    # svc = SVC(kernel='linear', probability=True, random_state=42)\n",
    "    svc = SVC(kernel='poly',degree=2,C=0.1,gamma=0.1,probability=True,random_state=42,class_weight='balanced') \n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "    log_reg = LogisticRegression(random_state=42, max_iter=1000)  # Added Logistic Regression\n",
    "\n",
    "    # # Voting classifier\n",
    "    # voting_clf = VotingClassifier(\n",
    "    #     estimators=[('SVM', svc), ('RF', rf)],\n",
    "    #     voting='soft'\n",
    "    # )\n",
    "\n",
    "    # Train classifiers\n",
    "    # voting_clf.fit(X_train, y_train)\n",
    "    svc.fit(X_train, y_train)\n",
    "    rf.fit(X_train, y_train)\n",
    "    log_reg.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate classifiers\n",
    "    # voting_preds = voting_clf.predict(X_test)\n",
    "    svc_preds = svc.predict(X_test)\n",
    "    rf_preds = rf.predict(X_test)\n",
    "    log_reg_preds = log_reg.predict(X_test)\n",
    "\n",
    "    # Print accuracies\n",
    "    # print(\"Voting Classifier Accuracy:\", accuracy_score(y_test, voting_preds))\n",
    "    print(\"SVM Accuracy:\", accuracy_score(y_test, svc_preds))\n",
    "    print(\"Random Forest Accuracy:\", accuracy_score(y_test, rf_preds))\n",
    "    print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, log_reg_preds))\n",
    "\n",
    "    return svc,rf,log_reg,scaler\n",
    "\n",
    "# Specify the folder containing your CSV files\n",
    "folder_path = \"./csvs\"  # Replace with your folder path if different\n",
    "\n",
    "# Load, balance, and combine the datasets\n",
    "dataframes = load_csvs_from_folder(folder_path)\n",
    "# balanced_data = balance_to_avg(dataframes, target_size=500)\n",
    "\n",
    "# Train and evaluate classifiers\n",
    "svc,rf,log_reg,scaler=train_classifiers(dataframes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define classifiers\n",
    "clf1 = LinearDiscriminantAnalysis()\n",
    "clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "clf4= SVC(kernel='poly',degree=2,C=0.1,gamma=0.1) \n",
    "\n",
    "# Specify the folder containing your CSV files\n",
    "folder_path = \"./csvs\"  # Replace with your folder path if different\n",
    "\n",
    "# Load, balance, and combine the datasets\n",
    "dataframes = load_csvs_from_folder(folder_path)\n",
    "# balanced_data = balance_to_avg(dataframes, target_size=500)\n",
    "classifiers = [('LDA', clf1), ('RF', clf2), ('GNB', clf3),('SVC', clf4)]\n",
    "X = dataframes.iloc[:, :-1].values  # All columns except the last one\n",
    "y = dataframes.iloc[:, -1].values   # Last column as target\n",
    "\n",
    "# Convert labels to numeric if necessary\n",
    "if y.dtype == 'object':\n",
    "    y = y.astype(int)\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# 3. Train and evaluate each classifier individually using cross-validation\n",
    "for name, clf in classifiers:\n",
    "    scores = cross_val_score(clf, X, y, cv=5)  \n",
    "    print(f\"{name} Accuracy: {np.mean(scores):.4f}\")\n",
    "\n",
    "# 4. Hard voting classifier\n",
    "eclf1 = VotingClassifier(estimators=classifiers, voting='hard')\n",
    "\n",
    "# 5. Evaluate the hard voting classifier using cross-validation\n",
    "scores = cross_val_score(eclf1, X, y, cv=5)  \n",
    "print(f\"Hard Voting Classifier Accuracy: {np.mean(scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Split data into test and validation sets\n",
    "X_train_new, X_val, y_train_new, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "# Step 2: Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'degree': [1,2],  # Test different polynomial degrees\n",
    "    'C': [0.01,0.1, 1],  # Test different values for regularization parameter C\n",
    "    'gamma': [0.01,0.1, 1]  # Test different gamma values\n",
    "}\n",
    "\n",
    "# Step 3: Create the SVM model\n",
    "svm_model = SVC(kernel='poly')\n",
    "\n",
    "# Step 4: Use GridSearchCV for parameter tuning\n",
    "grid_search = GridSearchCV(svm_model, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_val, y_val)\n",
    "\n",
    "# Step 5: Get the best parameters and retrain the model\n",
    "best_params = grid_search.best_params_\n",
    "best_svm_model = SVC(kernel='poly', **best_params)\n",
    "best_svm_model.fit(X_train, y_train)  # Retrain on the entire X_train\n",
    "\n",
    "# Step 6: Evaluate the final model on the test set\n",
    "test_accuracy = best_svm_model.score(X_test, y_test)\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Test Accuracy:\",test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save models and scaler to .pkl files\n",
    "# with open('./models/voting_clf.pkl', 'wb') as f:\n",
    "#     pickle.dump(voting_clf, f)\n",
    "\n",
    "with open('./modelsNew/svc.pkl', 'wb') as f:\n",
    "    pickle.dump(svc, f)\n",
    "\n",
    "with open('./modelsNew/rf.pkl', 'wb') as f:\n",
    "    pickle.dump(rf, f)\n",
    "\n",
    "with open('./modelsNew/log_reg.pkl', 'wb') as f:\n",
    "    pickle.dump(log_reg, f)\n",
    "\n",
    "with open('./modelsNew/scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(\"Models and scaler saved successfully using pickle!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models and scaler from .pkl files\n",
    "# with open('./models/voting_clf.pkl', 'rb') as f:\n",
    "#     voting_clf = pickle.load(f)\n",
    "\n",
    "with open('./modelsNew/svc.pkl', 'rb') as f:\n",
    "    svc = pickle.load(f)\n",
    "\n",
    "with open('./modelsNew/rf.pkl', 'rb') as f:\n",
    "    rf = pickle.load(f)\n",
    "\n",
    "with open('./modelsNew/log_reg.pkl', 'rb') as f:\n",
    "    log_reg = pickle.load(f)\n",
    "\n",
    "with open('./modelsNew/scaler.pkl', 'rb') as f:\n",
    "    scaler = pickle.load(f)\n",
    "\n",
    "print(\"Models and scaler loaded successfully using pickle!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a new input image\n",
    "def predict_image(image_path, svc, rf, log_reg, scaler):\n",
    "     # Load the image\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Use grayscale for HOG\n",
    "    if img is None:\n",
    "        print(f\"Failed to load image: {image_path}\")\n",
    "\n",
    "    # # Convert the grayscale image to binary using thresholding\n",
    "    # _, binary_img = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)\n",
    "    # print(binary_img)\n",
    "    inverted_img=img\n",
    "\n",
    "    # # Check if the background (top-left pixel) is dark\n",
    "    background_pixel = img[0, 0]\n",
    "    if background_pixel <128:  # Background is dark\n",
    "        print(f\"Inverting image\")\n",
    "        inverted_img = 255-img\n",
    "\n",
    "    \n",
    "    show_images([inverted_img])\n",
    "    resized_image=resize_image(inverted_img)\n",
    "    \n",
    "    # Preprocess the image and extract features\n",
    "    features = extract_hog_features(resized_image)\n",
    "\n",
    "    # Standardize the features using the same scaler as during training\n",
    "    features = scaler.transform([features])\n",
    "\n",
    "    # Predict using each classifier\n",
    "    # voting_prediction = voting_clf.predict(features)[0]\n",
    "    svc_prediction = svc.predict(features)[0]\n",
    "    rf_prediction = rf.predict(features)[0]\n",
    "    log_reg_prediction = log_reg.predict(features)[0]\n",
    "\n",
    "    return {\n",
    "        # \"Voting Classifier\": voting_prediction,\n",
    "        \"SVM\": svc_prediction,\n",
    "        \"Random Forest\": rf_prediction,\n",
    "        \"Logisitc Regression\": log_reg_prediction,\n",
    "    }\n",
    "\n",
    "for i in range(8,65):\n",
    "    image_path =  f\"./processing_output/{i}.png\"\n",
    "    predictions = predict_image(image_path,  svc, rf,log_reg, scaler)\n",
    "\n",
    "    print(\"\\nPredictions for the input image:\")\n",
    "    for classifier, prediction in predictions.items():\n",
    "        print(f\"{classifier}: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a mapping for predictions\n",
    "label_mapping = {\n",
    "    1: \"digit 1\",\n",
    "    2: \"digit 2\",\n",
    "    3: \"digit 3\",\n",
    "    4: \"digit 4\",\n",
    "    5: \"digit 5\",\n",
    "    6: \"digit 6\",\n",
    "    7: \"digit 7\",\n",
    "    8: \"digit 8\",\n",
    "    9: \"digit 9\",\n",
    "    11: \"a_1\",\n",
    "    12: \"a_2\",\n",
    "    14: \"a_4\",\n",
    "    18: \"a_8\",\n",
    "    16: \"a_16\",\n",
    "    32: \"a_32\",\n",
    "    33: \"double flat\",\n",
    "    34: \"double sharp\",\n",
    "    35: \"flat\",\n",
    "    36: \"natural\",\n",
    "    37: \"sharp\",\n",
    "    22: \"b_2\",\n",
    "    24: \"b_4\",\n",
    "    28: \"b_8\",\n",
    "    26: \"b_16\",\n",
    "    232: \"b_32\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
