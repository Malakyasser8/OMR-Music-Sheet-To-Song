{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from sklearn.utils import resample\n",
    "from matplotlib.pyplot import bar\n",
    "from skimage.exposure import histogram\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Show the figures / plots inside the notebook\n",
    "def show_images(images,titles=None):\n",
    "    #This function is used to show image(s) with titles by sending an array of images and an array of associated titles.\n",
    "    # images[0] will be drawn with the title titles[0] if exists\n",
    "    # You aren't required to understand this function, use it as-is.\n",
    "    n_ims = len(images)\n",
    "    if titles is None: titles = ['(%d)' % i for i in range(1,n_ims + 1)]\n",
    "    fig = plt.figure()\n",
    "    n = 1\n",
    "    for image,title in zip(images,titles):\n",
    "        a = fig.add_subplot(1,n_ims,n)\n",
    "        if image.ndim == 2: \n",
    "            plt.gray()\n",
    "        plt.imshow(image)\n",
    "        a.set_title(title)\n",
    "        n += 1\n",
    "    fig.set_size_inches(np.array(fig.get_size_inches()) * n_ims)\n",
    "    plt.show() \n",
    "\n",
    "\n",
    "def showHist(img):\n",
    "    # An \"interface\" to matplotlib.axes.Axes.hist() method\n",
    "    plt.figure()\n",
    "    imgHist = histogram(img, nbins=256)\n",
    "    \n",
    "    bar(imgHist[1].astype(np.uint8), imgHist[0], width=0.8, align='center')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_and_convert_images(folder_path):\n",
    "    # Get all files in the folder\n",
    "    files = os.listdir(folder_path)\n",
    "    \n",
    "    # Filter image files and sort them\n",
    "    image_files = [f for f in files if f.lower().endswith(('.png', '.bmp', '.jpg', '.jpeg'))]\n",
    "    image_files.sort()  # Optional: Ensures renaming follows sorted order\n",
    "\n",
    "    # Process each image\n",
    "    for idx, file_name in enumerate(image_files, start=1):\n",
    "        old_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        # Set new name with index and PNG extension\n",
    "        new_name = f\"{idx}.png\"\n",
    "        new_path = os.path.join(folder_path, new_name)\n",
    "        \n",
    "        try:\n",
    "            # Convert BMP to PNG and resize\n",
    "            img = cv2.imread(old_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if img is None:\n",
    "                print(f\"Failed to load image: {file_name}\")\n",
    "                continue\n",
    "            \n",
    "            background_pixel = img[0, 0]\n",
    "            if background_pixel < 128:  # Background is not white (dark)\n",
    "                # print(f\"Inverting background for {file_name}\")\n",
    "                img = cv2.bitwise_not(img)  # Invert the colors\n",
    "\n",
    "            # Resize to 32x32\n",
    "            resized_img = cv2.resize(img, (32, 32))\n",
    "            \n",
    "            # Save the resized image using PIL for PNG format\n",
    "            pil_img = Image.fromarray(resized_img)\n",
    "            pil_img.save(new_path, \"PNG\")\n",
    "            \n",
    "            # Remove the old BMP file\n",
    "            os.remove(old_path)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {e}\")\n",
    "            continue  # Skip to the next file if an error occurs\n",
    "\n",
    "    print(\"Renaming, resizing, and conversion completed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hog_features(img):\n",
    "    # Ensure input is a NumPy array\n",
    "    if img is None or not isinstance(img, np.ndarray):\n",
    "        raise ValueError(\"Invalid input image. Ensure it's loaded correctly.\")\n",
    "    # Resize image to target size\n",
    "    img = cv2.resize(img, (32, 32))\n",
    "    # show_images([img])\n",
    "    \n",
    "    # Define HOG descriptor parameters\n",
    "    win_size = (32, 32)\n",
    "    cell_size = (4, 4)\n",
    "    block_size_in_cells = (2, 2)\n",
    "    block_size = (block_size_in_cells[1] * cell_size[1],\n",
    "                  block_size_in_cells[0] * cell_size[0])\n",
    "    block_stride = (cell_size[1], cell_size[0])\n",
    "    nbins = 9\n",
    "    \n",
    "    # Initialize HOG descriptor\n",
    "    hog = cv2.HOGDescriptor(win_size, block_size, block_stride, cell_size, nbins)\n",
    "    \n",
    "    # Compute HOG features\n",
    "    h = hog.compute(img)\n",
    "    return h.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images_in_folder(label_folder, label, output_csv=\"features_with_labels.csv\"):\n",
    "    # Check if the CSV file exists\n",
    "    file_exists = os.path.isfile(output_csv)\n",
    "    \n",
    "    with open(output_csv, \"a\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        # Write header if the file doesn't exist\n",
    "        if not file_exists:\n",
    "            num_features = 1764  # Adjust to match your HOG settings\n",
    "            header = [f\"x{i+1}\" for i in range(num_features)] + [\"y\"]\n",
    "            writer.writerow(header)\n",
    "            print(f\"Created {output_csv} and added header.\")\n",
    "        \n",
    "        # Iterate through all image files in the label folder\n",
    "        for file_name in os.listdir(label_folder):\n",
    "            file_path = os.path.join(label_folder, file_name)\n",
    "            \n",
    "            # Skip non-image files\n",
    "            if not file_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "                continue\n",
    "            \n",
    "            # Load the image\n",
    "            img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)  # Use grayscale for HOG\n",
    "            if img is None:\n",
    "                print(f\"Failed to load image: {file_name}\")\n",
    "                continue\n",
    "            \n",
    "            # Extract HOG features\n",
    "            features = extract_hog_features(img)\n",
    "            # print(len(features))\n",
    "            # Append the label and write to the CSV\n",
    "            writer.writerow(np.append(features, label))\n",
    "            print(f\"Processed: {file_name} with label {label}\")\n",
    "\n",
    "    print(f\"Features and labels saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Provide the folder path containing the images\n",
    "# folder_path = \"./dataset/digits_dataset/9\"\n",
    "# rename_and_convert_images(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# label_folder = \"./dataset/a_1\"\n",
    "# process_images_in_folder(label_folder,\"a_1\",\"a_1_features_with_labels.csv\")\n",
    "\n",
    "# label_folder = \"./dataset/a_2\"\n",
    "# process_images_in_folder(label_folder,\"a_2\",\"a_2_features_with_labels.csv\")\n",
    "\n",
    "# label_folder = \"./dataset/a_4\"\n",
    "# process_images_in_folder(label_folder,\"a_4\",\"a_4_features_with_labels.csv\")\n",
    "\n",
    "# label_folder = \"./dataset/a_8\"\n",
    "# process_images_in_folder(label_folder,\"a_8\",\"a_8_features_with_labels.csv\")\n",
    "\n",
    "# label_folder = \"./dataset/a_16\"\n",
    "# process_images_in_folder(label_folder,\"a_16\",\"a_16_features_with_labels.csv\")\n",
    "\n",
    "# label_folder = \"./dataset/a_32\"\n",
    "# process_images_in_folder(label_folder,\"a_32\",\"a_32_features_with_labels.csv\")\n",
    "\n",
    "# label_folder = \"./dataset/double_flat\"\n",
    "# process_images_in_folder(label_folder,\"double_flat\",\"double_flat_features_with_labels.csv\")\n",
    "\n",
    "# label_folder = \"./dataset/double_sharp\"\n",
    "# process_images_in_folder(label_folder,\"double_sharp\",\"double_sharp_features_with_labels.csv\")\n",
    "\n",
    "# label_folder = \"./dataset/flat\"\n",
    "# process_images_in_folder(label_folder,\"flat\",\"flat_features_with_labels.csv\")\n",
    "\n",
    "# label_folder = \"./dataset/natural\"\n",
    "# process_images_in_folder(label_folder,\"natural\",\"natural_features_with_labels.csv\")\n",
    "\n",
    "# label_folder = \"./dataset/sharp\"\n",
    "# process_images_in_folder(label_folder,\"sharp\",\"sharp_features_with_labels.csv\")\n",
    "\n",
    "# label_folder = \"./dataset/digits_dataset/9\"\n",
    "# process_images_in_folder(label_folder,\"9\",\"9_features_with_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a mapping for predictions\n",
    "label_mapping = {\n",
    "    1: \"digit 1\",\n",
    "    2: \"digit 2\",\n",
    "    3: \"digit 3\",\n",
    "    4: \"digit 4\",\n",
    "    5: \"digit 5\",\n",
    "    6: \"digit 6\",\n",
    "    7: \"digit 7\",\n",
    "    8: \"digit 8\",\n",
    "    9: \"digit 9\",\n",
    "    11: \"a_1\",\n",
    "    12: \"a_2\",\n",
    "    14: \"a_4\",\n",
    "    18: \"a_8\",\n",
    "    16: \"a_16\",\n",
    "    32: \"a_32\",\n",
    "    33: \"double flat\",\n",
    "    34: \"double sharp\",\n",
    "    35: \"flat\",\n",
    "    36: \"natural\",\n",
    "    37: \"sharp\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train\n",
      "[[-0.3286457  -0.29321716 -0.25324788 ... -0.25281309 -0.28537911\n",
      "  -0.52145781]\n",
      " [-0.39235534 -0.29321716 -0.27485905 ... -0.25281309 -0.25386744\n",
      "   1.92391937]\n",
      " [-0.39235534 -0.29321716 -0.27485905 ... -0.25281309 -0.28537911\n",
      "  -0.11374549]\n",
      " ...\n",
      " [-0.39235534 -0.29321716 -0.27485905 ... -0.25281309 -0.28537911\n",
      "  -0.52145781]\n",
      " [-0.39235534 -0.29321716 -0.27485905 ... -0.25281309 -0.28537911\n",
      "   0.27384111]\n",
      " [-0.39235534 -0.29321716 -0.27485905 ... -0.25281309 -0.28537911\n",
      "  -0.52145781]]\n",
      "Voting Classifier Accuracy: 0.9835\n",
      "SVM Accuracy: 0.984\n",
      "Random Forest Accuracy: 0.962\n",
      "Logistic Regression Accuracy: 0.9675\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1: Load All CSV Files from a Folder\n",
    "def load_csvs_from_folder(folder_path):\n",
    "    csv_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "    dataframes = [pd.read_csv(csv_file) for csv_file in csv_files]\n",
    "    return dataframes\n",
    "\n",
    "# Step 2: Balance Each Dataset to Target Size\n",
    "def balance_to_avg(dataframes, target_size=800):\n",
    "    balanced_dataframes = []\n",
    "    for df in dataframes:\n",
    "        if len(df) < target_size:\n",
    "            # Augment smaller datasets by resampling\n",
    "            augmented_df = resample(\n",
    "                df, \n",
    "                replace=True, \n",
    "                n_samples=target_size, \n",
    "                random_state=42\n",
    "            )\n",
    "            balanced_dataframes.append(augmented_df)\n",
    "        else:\n",
    "            # Downsample larger datasets\n",
    "            downsampled_df = df.sample(n=target_size, random_state=42)\n",
    "            balanced_dataframes.append(downsampled_df)\n",
    "    return pd.concat(balanced_dataframes)\n",
    "\n",
    "# Step 3: Train and Evaluate Classifiers\n",
    "def train_classifiers(data):\n",
    "    # Separate features and labels\n",
    "    X = data.iloc[:, :-1].values  # All columns except the last one\n",
    "    y = data.iloc[:, -1].values   # Last column as target\n",
    "    \n",
    "    # Convert labels to numeric if necessary\n",
    "    if y.dtype == 'object':\n",
    "        y = y.astype(int)\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    # Split into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    print(\"X_train\")\n",
    "    print(X_train)\n",
    "    # Define classifiers\n",
    "    # svc = SVC(kernel='linear', probability=True, random_state=42)\n",
    "    svc = SVC(kernel='poly',degree=2,C=0.1,gamma=0.1,probability=True,random_state=42) \n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "    log_reg = LogisticRegression(random_state=42, max_iter=1000)  # Added Logistic Regression\n",
    "\n",
    "    # Voting classifier\n",
    "    voting_clf = VotingClassifier(\n",
    "        estimators=[('SVM', svc), ('RF', rf)],\n",
    "        voting='soft'\n",
    "    )\n",
    "\n",
    "    # Train classifiers\n",
    "    voting_clf.fit(X_train, y_train)\n",
    "    svc.fit(X_train, y_train)\n",
    "    rf.fit(X_train, y_train)\n",
    "    log_reg.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate classifiers\n",
    "    voting_preds = voting_clf.predict(X_test)\n",
    "    svc_preds = svc.predict(X_test)\n",
    "    rf_preds = rf.predict(X_test)\n",
    "    log_reg_preds = log_reg.predict(X_test)\n",
    "\n",
    "    # Print accuracies\n",
    "    print(\"Voting Classifier Accuracy:\", accuracy_score(y_test, voting_preds))\n",
    "    print(\"SVM Accuracy:\", accuracy_score(y_test, svc_preds))\n",
    "    print(\"Random Forest Accuracy:\", accuracy_score(y_test, rf_preds))\n",
    "    print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, log_reg_preds))\n",
    "\n",
    "    return voting_clf,svc,rf,log_reg,scaler\n",
    "\n",
    "# Specify the folder containing your CSV files\n",
    "folder_path = \"./csvs\"  # Replace with your folder path if different\n",
    "\n",
    "# Load, balance, and combine the datasets\n",
    "dataframes = load_csvs_from_folder(folder_path)\n",
    "balanced_data = balance_to_avg(dataframes, target_size=500)\n",
    "\n",
    "# Train and evaluate classifiers\n",
    "voting_clf,svc,rf,log_reg,scaler=train_classifiers(balanced_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m folder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./csvs\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with your folder path if different\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Load, balance, and combine the datasets\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m dataframes \u001b[38;5;241m=\u001b[39m \u001b[43mload_csvs_from_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m balanced_data \u001b[38;5;241m=\u001b[39m balance_to_avg(dataframes, target_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m)\n\u001b[0;32m     13\u001b[0m classifiers \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLDA\u001b[39m\u001b[38;5;124m'\u001b[39m, clf1), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRF\u001b[39m\u001b[38;5;124m'\u001b[39m, clf2), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGNB\u001b[39m\u001b[38;5;124m'\u001b[39m, clf3),(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSVC\u001b[39m\u001b[38;5;124m'\u001b[39m, clf4)]\n",
      "Input \u001b[1;32mIn [32]\u001b[0m, in \u001b[0;36mload_csvs_from_folder\u001b[1;34m(folder_path)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_csvs_from_folder\u001b[39m(folder_path):\n\u001b[0;32m      3\u001b[0m     csv_files \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_path, f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(folder_path) \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m----> 4\u001b[0m     dataframes \u001b[38;5;241m=\u001b[39m [pd\u001b[38;5;241m.\u001b[39mread_csv(csv_file) \u001b[38;5;28;01mfor\u001b[39;00m csv_file \u001b[38;5;129;01min\u001b[39;00m csv_files]\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataframes\n",
      "Input \u001b[1;32mIn [32]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_csvs_from_folder\u001b[39m(folder_path):\n\u001b[0;32m      3\u001b[0m     csv_files \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_path, f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(folder_path) \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m----> 4\u001b[0m     dataframes \u001b[38;5;241m=\u001b[39m [\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_file\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m csv_file \u001b[38;5;129;01min\u001b[39;00m csv_files]\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataframes\n",
      "File \u001b[1;32mc:\\Users\\malak\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\malak\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\malak\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\malak\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "# 2. Define classifiers\n",
    "clf1 = LinearDiscriminantAnalysis()\n",
    "clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "clf4= SVC(kernel='poly',degree=2,C=0.1,gamma=0.1) \n",
    "\n",
    "# Specify the folder containing your CSV files\n",
    "folder_path = \"./csvs\"  # Replace with your folder path if different\n",
    "\n",
    "# Load, balance, and combine the datasets\n",
    "dataframes = load_csvs_from_folder(folder_path)\n",
    "balanced_data = balance_to_avg(dataframes, target_size=500)\n",
    "classifiers = [('LDA', clf1), ('RF', clf2), ('GNB', clf3),('SVC', clf4)]\n",
    "X = balanced_data.iloc[:, :-1].values  # All columns except the last one\n",
    "y = balanced_data.iloc[:, -1].values   # Last column as target\n",
    "\n",
    "# Convert labels to numeric if necessary\n",
    "if y.dtype == 'object':\n",
    "    y = y.astype(int)\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# 3. Train and evaluate each classifier individually using cross-validation\n",
    "for name, clf in classifiers:\n",
    "    scores = cross_val_score(clf, X, y, cv=5)  \n",
    "    print(f\"{name} Accuracy: {np.mean(scores):.4f}\")\n",
    "\n",
    "# 4. Hard voting classifier\n",
    "eclf1 = VotingClassifier(estimators=classifiers, voting='hard')\n",
    "\n",
    "# 5. Evaluate the hard voting classifier using cross-validation\n",
    "scores = cross_val_score(eclf1, X, y, cv=5)  \n",
    "print(f\"Hard Voting Classifier Accuracy: {np.mean(scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 0.01, 'degree': 2, 'gamma': 0.1}\n",
      "Test Accuracy: 0.984\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Split data into test and validation sets\n",
    "X_train_new, X_val, y_train_new, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "# Step 2: Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'degree': [1,2],  # Test different polynomial degrees\n",
    "    'C': [0.01,0.1, 1],  # Test different values for regularization parameter C\n",
    "    'gamma': [0.01,0.1, 1]  # Test different gamma values\n",
    "}\n",
    "\n",
    "# Step 3: Create the SVM model\n",
    "svm_model = SVC(kernel='poly')\n",
    "\n",
    "# Step 4: Use GridSearchCV for parameter tuning\n",
    "grid_search = GridSearchCV(svm_model, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_val, y_val)\n",
    "\n",
    "# Step 5: Get the best parameters and retrain the model\n",
    "best_params = grid_search.best_params_\n",
    "best_svm_model = SVC(kernel='poly', **best_params)\n",
    "best_svm_model.fit(X_train, y_train)  # Retrain on the entire X_train\n",
    "\n",
    "# Step 6: Evaluate the final model on the test set\n",
    "test_accuracy = best_svm_model.score(X_test, y_test)\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Test Accuracy:\",test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models and scaler saved successfully using pickle!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save models and scaler to .pkl files\n",
    "with open('./models/voting_clf.pkl', 'wb') as f:\n",
    "    pickle.dump(voting_clf, f)\n",
    "\n",
    "with open('./models/svc.pkl', 'wb') as f:\n",
    "    pickle.dump(svc, f)\n",
    "\n",
    "with open('./models/rf.pkl', 'wb') as f:\n",
    "    pickle.dump(rf, f)\n",
    "\n",
    "with open('./models/log_reg.pkl', 'wb') as f:\n",
    "    pickle.dump(log_reg, f)\n",
    "\n",
    "with open('./models/scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(\"Models and scaler saved successfully using pickle!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models and scaler loaded successfully using pickle!\n"
     ]
    }
   ],
   "source": [
    "# Load models and scaler from .pkl files\n",
    "with open('./models/voting_clf.pkl', 'rb') as f:\n",
    "    voting_clf = pickle.load(f)\n",
    "\n",
    "with open('./models/svc.pkl', 'rb') as f:\n",
    "    svc = pickle.load(f)\n",
    "\n",
    "with open('./models/rf.pkl', 'rb') as f:\n",
    "    rf = pickle.load(f)\n",
    "\n",
    "with open('./models/log_reg.pkl', 'rb') as f:\n",
    "    log_reg = pickle.load(f)\n",
    "\n",
    "with open('./models/scaler.pkl', 'rb') as f:\n",
    "    scaler = pickle.load(f)\n",
    "\n",
    "print(\"Models and scaler loaded successfully using pickle!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions for the input image:\n",
      "Voting Classifier: 32\n",
      "SVM: 32\n",
      "Random Forest: 16\n",
      "Logisitc Regression: 16\n"
     ]
    }
   ],
   "source": [
    "# Test on a new input image\n",
    "def predict_image(image_path, voting_clf, svc, rf, log_reg, scaler):\n",
    "     # Load the image\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Use grayscale for HOG\n",
    "    if img is None:\n",
    "        print(f\"Failed to load image: {image_path}\")\n",
    "        \n",
    "    # Preprocess the image and extract features\n",
    "    features = extract_hog_features(img)\n",
    "\n",
    "    # Standardize the features using the same scaler as during training\n",
    "    features = scaler.transform([features])\n",
    "\n",
    "    # Predict using each classifier\n",
    "    voting_prediction = voting_clf.predict(features)[0]\n",
    "    svc_prediction = svc.predict(features)[0]\n",
    "    rf_prediction = rf.predict(features)[0]\n",
    "    log_reg_prediction = log_reg.predict(features)[0]\n",
    "\n",
    "    return {\n",
    "        \"Voting Classifier\": voting_prediction,\n",
    "        \"SVM\": svc_prediction,\n",
    "        \"Random Forest\": rf_prediction,\n",
    "        \"Logisitc Regression\": log_reg_prediction,\n",
    "    }\n",
    "\n",
    "image_path = \"test_image.png\"  # Replace with your test image path\n",
    "predictions = predict_image(image_path, voting_clf, svc, rf,log_reg, scaler)\n",
    "\n",
    "print(\"\\nPredictions for the input image:\")\n",
    "for classifier, prediction in predictions.items():\n",
    "    print(f\"{classifier}: {prediction}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
